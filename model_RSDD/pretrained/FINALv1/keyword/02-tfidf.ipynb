{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASKER KEYWORD - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# LogOddsRatio Class\n",
    "# \n",
    "# A class for computing Log-odds-ratio with informative Dirichlet priors\n",
    "#\n",
    "# See http://languagelog.ldc.upenn.edu/myl/Monroe.pdf for more detail\n",
    "# \n",
    "#####################################################################\n",
    "\n",
    "__author__ = \"Kornraphop Kawintiranon\"\n",
    "__email__ = \"kornraphop.k@gmail.com\"\n",
    "\n",
    "import math\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import json, pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizerFast, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "# from src.models  import *\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_stopwords      = True\n",
    "rm_punctuations   = True\n",
    "save_top_words    = 3000\n",
    "\n",
    "lower_case        = False # already DONE\n",
    "tokenizer         = None # use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ratio = 1\n",
    "\n",
    "depression_text = pickle.load(open(f\"../data/classi/classi_corpus_traindep_ratio{classi_ratio}.pkl\", \"rb\"))\n",
    "control_text    = pickle.load(open(f\"../data/classi/classi_corpus_traincon_ratio{classi_ratio}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "\n",
    "def parallel_tokenize(corpus, tokenizer=None, n_jobs=-1):\n",
    "    if tokenizer == None:\n",
    "        tokenizer = NLTKWordTokenizer()\n",
    "    if n_jobs < 0:\n",
    "        n_jobs = multiprocessing.cpu_count() - 1\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        corpus_tokenized = list(\n",
    "            tqdm.tqdm(executor.map(tokenizer.tokenize, corpus, chunksize=200), total=len(corpus), desc='Tokenizing')\n",
    "        )\n",
    "    return corpus_tokenized\n",
    "\n",
    "def remove_stopwords(corpus, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        \n",
    "        # print(words[:100])\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # print(words[:100])\n",
    "        # asdfasfasdf\n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "\n",
    "def remove_punctuations(corpus):\n",
    "    punctuations = string.punctuation\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        # remove single punctuations\n",
    "        words = [w for w in words if not w in punctuations]\n",
    "        words = [re.sub(r\"\"\"[()#[\\]#*+\\-/:;<=>@[\\]^_`{|}~\"\\\\.?!$%&]\"\"\", \"\", w) for w in words]      \n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "    \n",
    "def decontract(corpus):\n",
    "    processed_corpus = []\n",
    "    for phrase in tqdm.tqdm(corpus, desc=\"Decontracting\"):\n",
    "        phrase = re.sub(r\"â€™\", \"\\'\", phrase)\n",
    "\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "        processed_corpus.append(phrase)\n",
    "    return processed_corpus\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    if lower_case:\n",
    "        print(\"lowercasing\")\n",
    "        corpus = [text.lower() for text in corpus]\n",
    "    corpus = decontract(corpus)\n",
    "    tokenized_corpus = parallel_tokenize(corpus, tokenizer)\n",
    "\n",
    "    if rm_stopwords:\n",
    "        print(\"removing stopwords\")\n",
    "        tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "        \n",
    "    if rm_punctuations:\n",
    "        print(\"removing punctuation\")\n",
    "        tokenized_corpus = remove_punctuations(tokenized_corpus)\n",
    "        \n",
    "    print(tokenized_corpus[0][:500])\n",
    "\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_text = preprocessing(depression_text)\n",
    "control_text    = preprocessing(control_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print(len(depression_text))\n",
    "print(len(control_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# combine the data into 2 document, one document for depression and one document for control group\n",
    "def get_tfidf_keyword(depression_text, control_text):\n",
    "\n",
    "    class_docs = [''] * 2  # concat all texts for each class\n",
    "    \n",
    "    # control = class 0\n",
    "    for text in control_text:\n",
    "        for word in text:\n",
    "            word = word + ' '\n",
    "            class_docs[0] += word\n",
    "    print(len(class_docs[0]))\n",
    "    \n",
    "    # depression = class 1\n",
    "    for text in depression_text:\n",
    "        for word in text:\n",
    "            word = word + ' '\n",
    "            class_docs[1] += word\n",
    "    print(len(class_docs[1]))\n",
    "\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    feat  = tfidf.fit_transform(class_docs).todense()  # (n_classes, vocabs)\n",
    "    feat  = np.squeeze(np.asarray(feat))  # matrix -> array\n",
    "            \n",
    "    # ---------- Control ----------\n",
    "    con_sorted_idx = feat[0].argsort()[::-1]\n",
    "    control_keyword = [tfidf.get_feature_names()[idx] for idx in con_sorted_idx]\n",
    "    # for idx in sorted_idx:\n",
    "    #     word = tfidf.get_feature_names()[idx]\n",
    "    #     control_keyword.append(word)\n",
    "\n",
    "     # ---------- Depression ----------\n",
    "    dep_sorted_idx = feat[1].argsort()[::-1]\n",
    "    depression_keyword = [tfidf.get_feature_names()[idx] for idx in dep_sorted_idx]\n",
    "    # for idx in sorted_idx:\n",
    "    #     word = tfidf.get_feature_names()[idx]\n",
    "    #     depression_keyword.append(word)\n",
    "\n",
    "    return depression_keyword, control_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742882\n",
      "4697533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "depression_keyword, control_keyword = get_tfidf_keyword(depression_text, control_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # top 1000 in control not in depression\n",
    "# indep_notincon = [word for word in depression_keyword[:10000] if word not in control_keyword[:10000]]\n",
    "# incon_notindep = [word for word in control_keyword if word not in depression_keyword]\n",
    "        \n",
    "# top_indep_notincon = indep_notincon[:1500]\n",
    "# top_incon_notindep = incon_notindep[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"./02-tfidf-depcon{save_top_words}-R{classi_ratio}-nostops.txt\", \"w\") as f:\n",
    "#     for word in top_indep_notincon:\n",
    "#         f.write(word + \"\\n\")\n",
    "#     for word in top_incon_notindep:\n",
    "#         f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "['lsd', 'dose', 'leviticus', 'mdma', 'pregnancy', 'tripping', 'meds', 'substance', 'tolerance', 'shrooms']\n"
     ]
    }
   ],
   "source": [
    "indep_notincon = [word for word in depression_keyword[:5477] if word not in control_keyword[:5477]]\n",
    "\n",
    "print(len(indep_notincon))\n",
    "print(indep_notincon[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "['cmbeezy', 'el', 'en', 'es', 'shipped', 'supreme', 'turgle', 'mitch', 'kosovo', 'capitalism']\n"
     ]
    }
   ],
   "source": [
    "incon_notindep = [word for word in control_keyword[:5477] if word not in depression_keyword[:5477]]\n",
    "\n",
    "print(len(incon_notindep))\n",
    "print(incon_notindep[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./02-tfidf-depcon{save_top_words}-R{classi_ratio}-nostops.txt\", \"w\") as f:\n",
    "    for word in indep_notincon:\n",
    "        f.write(word + \"\\n\")\n",
    "    for word in incon_notindep:\n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54ebe46dc67c0b0016e368835037c988a8dce633f341e79a61a84613b212514"
  },
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
