{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bdc0cb-f4a9-4245-941c-d6d192f0d951",
   "metadata": {},
   "source": [
    "# Check what the Neural Network method mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b52a1c6-ebae-464c-8979-51549d7879aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "from src.traineval  import *\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845b425-1110-4e06-9fc8-a8dc5080276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open(f'../data/domain/domainchunk-R10-test-ds.pkl', \"rb\"))\n",
    "\n",
    "for sample in train_dataset:\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['word_ids'])\n",
    "    print(sample['attention_mask'])\n",
    "    print(sample['orig_text'])\n",
    "    print(sample['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cab94e-9370-442b-874c-8fce84d4a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# can shuffle now because we use the model to do inference on any sample\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6993de8e-ef55-4cd1-ac1b-5ce222dd51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from :  ../save/NN-02-classitoken-round2/best-model-4200.tar\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "num_labels  = 2 # CE\n",
    "checkpoint = 'bert-base-uncased'\n",
    "model       = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels = num_labels).to(device)\n",
    "model.classifier.dropout = nn.Dropout(p = 0.1, inplace = False)\n",
    "\n",
    "# load model from trained model\n",
    "path = '../save/NN-02-classitoken-round2/best-model-4200.tar'\n",
    "print(\"Load model from : \", path)\n",
    "\n",
    "loaded_checkpoint = torch.load(path)\n",
    "model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
    "print(model.load_state_dict(loaded_checkpoint['model_state_dict'])) # <All keys matched successfully>\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer     = BertTokenizerFast.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41fe5e7f-77f3-4f58-a498-73f580501294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNMLMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_loader, classitoken_model, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_mask_id = self.tokenizer(self.tokenizer.mask_token)['input_ids'][1] \n",
    "        self.model_cls_id     = self.tokenizer(self.tokenizer.cls_token)['input_ids'][1]\n",
    "        self.model_sep_id     = self.tokenizer(self.tokenizer.sep_token)['input_ids'][1]\n",
    "        self.make_NN_MLM_ds(data_loader, classitoken_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        del classitoken_model, data_loader\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):       \n",
    "        sample = {  'input_ids'      : self.list_input_ids[idx],\n",
    "                    'word_ids'       : self.list_word_ids[idx],\n",
    "                    'attention_mask' : self.list_attention_mask[idx],\n",
    "                    'orig_text'      : self.list_orig_text[idx],\n",
    "                    'masked_text'    : self.list_masked_text[idx],\n",
    "                    'labels'         : self.list_labels[idx]}\n",
    "        return sample\n",
    "    \n",
    "    # Use the trained model to do inference on domain dataset to creats masked domain ds\n",
    "    def make_NN_MLM_ds(self, data_loader, model):\n",
    "        \n",
    "        self.list_input_ids      = []\n",
    "        self.list_word_ids       = []\n",
    "        self.list_attention_mask = []\n",
    "        self.list_orig_text      = []\n",
    "        self.list_masked_text    = []\n",
    "        self.list_labels         = []\n",
    "        \n",
    "        self.list_important_input_ids = []\n",
    "        # self.list_important_word_ids = []\n",
    "        # self.list_important_idx_seq = []\n",
    "        # self.list_important_idx_pos = []\n",
    "\n",
    "        for idx, batch in enumerate(data_loader):  \n",
    "            \n",
    "            sys.stdout.write(str(idx))\n",
    "            \n",
    "            input_ids = batch['input_ids'].clone().to(device)\n",
    "            att_mask  = batch['attention_mask'].clone().to(device)\n",
    "            \n",
    "            word_ids     = batch['word_ids']\n",
    "            orig_text    = batch['orig_text']\n",
    "            labels       = batch['labels']\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids, attention_mask = att_mask)\n",
    "            \n",
    "            pred = torch.argmax(torch.softmax(outputs.logits.detach(), dim = 2), dim = 2) # bs, seq_len\n",
    "            # print(pred.shape)\n",
    "            \n",
    "            # get index of important tokens\n",
    "            important_idx_seq = (pred == 1).nonzero(as_tuple=True)[0]\n",
    "            important_idx_pos = (pred == 1).nonzero(as_tuple=True)[1]\n",
    "            \n",
    "            # print(important_idx_seq[0:5], important_idx_pos[0:5])\n",
    "            \n",
    "            important_input_ids = input_ids.clone().detach()[important_idx_seq, important_idx_pos]\n",
    "#             important_word_ids = word_ids.clone().detach()[important_idx_seq, important_idx_pos]\n",
    "            \n",
    "#             # print(important_input_ids)\n",
    "            \n",
    "#             # put [MASK] token at the position of the important tokens\n",
    "#             masked_input_ids = input_ids.detach().clone()\n",
    "#             masked_input_ids[important_idx_seq, important_idx_pos] = self.model_mask_id\n",
    "#             # ensure that the first and last tokens are not masked\n",
    "#             masked_input_ids[:, 0]   = self.model_cls_id\n",
    "#             masked_input_ids[:, 511] = self.model_sep_id\n",
    "            \n",
    "#             # print(masked_input_ids.shape)\n",
    "            \n",
    "#             labels    = torch.ones_like(att_mask).to(device) * -100 # init all labels with -100\n",
    "#             # put original token input_ids at the position of important tokens\n",
    "#             masked_labels  = labels.index_put(indices = (important_idx_seq, important_idx_pos) , values = important_input_ids)\n",
    "#             # ensure that the model do not predict the fist and last tokens\n",
    "#             masked_labels[:, 0]   = -100\n",
    "#             masked_labels[:, 511] = -100\n",
    "            \n",
    "            self.list_important_input_ids.append(important_input_ids)\n",
    "            # self.list_important_word_ids.append(important_word_ids)\n",
    "            # self.list_important_idx_seq.append(important_idx_seq)\n",
    "            # self.list_important_idx_pos.append(important_idx_pos)\n",
    "            \n",
    "            # if idx == 10 :\n",
    "            #     break\n",
    "            \n",
    "#             for i in range(input_ids.shape[0]):\n",
    "            \n",
    "#                 self.list_input_ids.append(masked_input_ids[i].clone())\n",
    "#                 self.list_word_ids.append(word_ids[i].clone())\n",
    "#                 self.list_attention_mask.append(torch.ones_like(masked_input_ids[i]))\n",
    "#                 self.list_orig_text.append(orig_text[i])\n",
    "#                 self.list_masked_text.append(self.tokenizer.decode(masked_input_ids[i].clone()))\n",
    "#                 self.list_labels.append(masked_labels[i].clone())\n",
    "                \n",
    "#                 # print(masked_input_ids[10].clone())\n",
    "#                 # print(masked_labels[10].clone())\n",
    "                \n",
    "#         assert len(self.list_input_ids) == len(self.list_word_ids) == len(self.list_attention_mask) == len(self.list_orig_text) == len(self.list_masked_text) == len(self.list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ed42f4-421b-4a75-946c-35af543b6452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012345678910"
     ]
    }
   ],
   "source": [
    "my_train_dataset   = NNMLMDataset(train_loader, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c913ab3-9b07-4385-aa03-b61a287607bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_token_freq = {}\n",
    "\n",
    "for input_ids in my_train_dataset.list_important_input_ids:\n",
    "    for each_id in input_ids:\n",
    "        # print(each_id)\n",
    "        token = tokenizer.decode(each_id)\n",
    "        # print(token)\n",
    "        if token not in masked_token_freq.keys():\n",
    "            # print(\"new\")\n",
    "            masked_token_freq[token] = 1\n",
    "        else : \n",
    "            # print(\"old\")\n",
    "            masked_token_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abb31fd-bb96-4002-a559-c5753667e601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 45), ('i', 41), ('that', 29), ('is', 29), ('and', 27), ('it', 26), ('to', 23), ('in', 20), ('a', 19), ('of', 18), ('for', 17), ('the', 16), ('this', 16), ('if', 16), (',', 16), ('u', 16), (\"'\", 15), ('are', 15), ('have', 15), ('t', 15), ('can', 14), ('there', 13), ('me', 13), ('with', 13), ('not', 13), ('ru', 13), ('##u', 12), ('nu', 12), ('up', 11), ('##s', 10), ('try', 10), ('know', 10), ('on', 9), ('your', 9), ('time', 9), ('all', 9), ('but', 9), ('use', 9), ('just', 9), ('thanks', 9), ('was', 8), ('would', 8), ('level', 8), ('?', 8), ('some', 7), ('want', 7), ('also', 7), ('then', 7), ('##y', 7), ('like', 7), ('be', 7), ('so', 7), ('at', 7), ('##l', 7), ('##13', 7), ('will', 6), ('different', 6), ('from', 6), ('no', 6), ('my', 6), ('good', 6), ('people', 6), ('do', 6), ('##kk', 6), ('don', 6), ('when', 6), ('who', 6), ('thing', 6), ('ice', 6), ('let', 6), ('##31', 6), ('has', 5), ('think', 5), ('##i', 5), ('about', 5), ('or', 5), ('see', 5), ('one', 5), ('app', 5), ('make', 5), ('##on', 5), ('shit', 5), ('s', 5), ('its', 5), ('##we', 5), ('mo', 5), ('deposited', 5), ('did', 5), ('cream', 5), ('an', 5), ('m', 5), ('find', 4), ('am', 4), ('b', 4), ('we', 4), ('##da', 4), ('##le', 4), ('cl', 4), ('what', 4), ('they', 4), ('te', 4), ('##tri', 4), ('yes', 4), ('cy', 4), ('##dia', 4), ('##h', 4), ('re', 4), ('##op', 4), ('bad', 4), ('only', 4), ('##t', 4), ('##e', 4), ('them', 4), ('##pro', 4), ('313', 4), ('ball', 4), ('naive', 4), ('new', 3), ('yeah', 3), ('had', 3), ('code', 3), ('first', 3), ('check', 3), ('problem', 3), ('ios', 3), ('task', 3), ('things', 3), ('way', 3), ('much', 3), ('their', 3), ('because', 3), ('by', 3), ('##er', 3), ('res', 3), ('##ping', 3), ('holding', 3), ('help', 3), ('boot', 3), ('pu', 3), ('spin', 3), ('female', 3), ('biggest', 3), ('ve', 3), ('everyone', 3), ('##ers', 3), ('##vot', 3), ('##x', 3), ('back', 3), ('now', 3), ('why', 3), ('wr', 3), ('##ump', 3), ('as', 3), ('kind', 3), ('deck', 3), ('dark', 3), ('##rai', 3), ('trade', 3), ('iv', 3), ('hp', 3), ('life', 3), ('ch', 3), ('##lor', 3), ('##hyl', 3), ('weather', 3), ('knock', 3), ('clear', 3), ('sm', 3), ('##og', 3), ('power', 3), ('love', 3), ('southern', 3), ('tim', 3), ('##id', 3), ('off', 3), ('mattered', 3), ('##8', 2), ('last', 2), ('greece', 2), ('post', 2), ('thinking', 2), ('air', 2), ('gr', 2), ('remove', 2), ('well', 2), ('##path', 2), ('##ta', 2), ('other', 2), ('maintaining', 2), ('##fi', 2), ('gui', 2), ('anyway', 2), ('were', 2), ('more', 2), ('problems', 2), ('since', 2), ('probably', 2), ('still', 2), ('kb', 2), ('example', 2), ('sure', 2), ('substrate', 2), ('ss', 2), ('restore', 2), ('pol', 2), ('flip', 2), ('##sw', 2), ('##itch', 2), ('##boot', 2), ('tapping', 2), ('made', 2), ('mine', 2), ('won', 2), ('volume', 2), ('button', 2), ('##hea', 2), ('##is', 2), ('she', 2), ('meta', 2), ('snow', 2), ('##osi', 2), ('ign', 2), ('river', 2), ('seems', 2), ('over', 2), ('cards', 2), ('which', 2), ('luck', 2), ('shows', 2), ('isn', 2), ('get', 2), ('never', 2), ('np', 2), ('forget', 2), ('uc', 2), ('##ar', 2), ('##5', 2), ('##v', 2), ('##ak', 2), ('maybe', 2), ('man', 2), ('work', 2), ('should', 2), ('##ing', 2), ('while', 2), ('out', 2), ('event', 2), ('bill', 2), ('mean', 2), ('male', 2), ('friend', 2), ('how', 2), ('got', 2), ('##ian', 2), ('light', 2), ('reason', 2), ('stuff', 2), ('drop', 2), ('makes', 2), ('after', 2), ('those', 2), ('didn', 2), ('care', 2), ('guy', 2), ('others', 2), ('her', 2), ('whore', 2), ('though', 2), ('having', 2), ('sex', 2), ('##load', 2), ('game', 2), ('another', 2), ('economy', 2), ('scrap', 2), ('quiet', 2), ('grass', 2), ('proof', 2), ('please', 2), ('interested', 2), ('dia', 2), ('within', 2), ('family', 2), ('russell', 2), ('##1', 2), ('whip', 2), ('sid', 2), ('hatch', 2), ('island', 2), ('slot', 2), ('exist', 2), ('bells', 2), ('##ut', 2), ('##ed', 2), ('father', 2), ('high', 2), ('his', 2), ('watching', 2), ('faster', 2), ('ok', 2), ('value', 2), ('employers', 2), ('custody', 2), ('rock', 2), ('head', 2), ('31', 2), ('hydro', 2), ('pump', 2), ('##n', 2), ('##30', 2), ('imperfect', 2), ('bag', 2), ('f', 2), ('seem', 1), ('send', 1), ('email', 1), ('gt', 1), ('##a', 1), ('europe', 1), ('tang', 1), ('##iers', 1), ('tobacco', 1), ('usually', 1), ('af', 1), ('wah', 1), ('##za', 1), ('##ya', 1), ('laws', 1), ('turkey', 1), ('ban', 1), ('completely', 1), ('spice', 1), ('##d', 1), ('cha', 1), ('font', 1), ('tell', 1), ('shipping', 1), ('mind', 1), ('lou', 1), ('##lak', 1), ('##aki', 1), ('city', 1), ('chan', 1), ('##ia', 1), ('sap', 1), ('##hire', 1), ('hot', 1), ('shot', 1), ('rt', 1), ('guess', 1), ('sam', 1), ('##sari', 1), ('yu', 1), ('##p', 1), ('hint', 1), ('hook', 1), ('##ah', 1), ('leaking', 1), ('##om', 1), ('##met', 1), ('anywhere', 1), ('hose', 1), ('puff', 1), ('without', 1), ('bowl', 1), ('close', 1), ('hole', 1), ('hand', 1), ('suck', 1), ('pull', 1), ('leak', 1), ('somewhere', 1), ('under', 1), ('##pack', 1), ('launches', 1), ('projects', 1), ('sets', 1), ('needed', 1), ('manage', 1), ('tasks', 1), ('running', 1), ('sync', 1), ('##db', 1), ('migrate', 1), ('load', 1), ('starting', 1), ('background', 1), ('runner', 1), ('specific', 1), ('##lic', 1), ('##tation', 1), ('easy', 1), ('takes', 1), ('buttons', 1), ('every', 1), ('consuming', 1), ('actually', 1), ('worse', 1), ('experience', 1), ('logs', 1), ('terminal', 1), ('present', 1), ('discus', 1), ('##ii', 1), ('gaming', 1), ('cough', 1), ('included', 1), ('batch', 1), ('end', 1), ('arguing', 1), ('scheduled', 1), ('these', 1), ('weekends', 1), ('tournaments', 1), ('own', 1), ('scheduling', 1), ('single', 1), ('free', 1), ('weekend', 1), ('august', 1), ('case', 1), ('throw', 1), ('blizzard', 1), ('com', 1), ('##uni', 1), ('##ty', 1), ('logic', 1), ('maps', 1), ('published', 1), ('any', 1), ('data', 1), ('size', 1), ('settings', 1), ('general', 1), ('usage', 1), ('tap', 1), ('show', 1), ('match', 1), ('backup', 1), ('need', 1), ('reins', 1), ('##tal', 1), ('##ling', 1), ('install', 1), ('open', 1), ('change', 1), ('password', 1), ('semi', 1), ('everything', 1), ('deleted', 1), ('keeping', 1), ('##us', 1), ('control', 1), ('##cent', 1), ('default', 1), ('##ggle', 1), ('con', 1), ('##fer', 1), ('##mation', 1), ('pop', 1), ('assign', 1), ('action', 1), ('shut', 1), ('safe', 1), ('##mo', 1), ('##de', 1), ('correct', 1), ('act', 1), ('##iva', 1), ('##tor', 1), ('##lo', 1), ('jail', 1), ('##broken', 1), ('##up', 1), ('del', 1), ('##ete', 1), ('installed', 1), ('answers', 1), ('red', 1), ('always', 1), ('clench', 1), ('thumb', 1), ('put', 1), ('##oin', 1), ('##ori', 1), ('##th', 1), ('##aldo', 1), ('##bas', 1), ('milo', 1), ('##tic', 1), ('cast', 1), ('##form', 1), ('shu', 1), ('##ppet', 1), ('bane', 1), ('##tte', 1), ('##ull', 1), ('##ps', 1), ('##ius', 1), ('g', 1), ('##ie', 1), ('##in', 1), ('##amp', 1), ('##ail', 1), ('##ant', 1), ('lu', 1), ('##c', 1), ('##gon', 1), ('sal', 1), ('##amen', 1), ('##ce', 1), ('bel', 1), ('##du', 1), ('##m', 1), ('##ng', 1), ('##oss', 1), ('##iro', 1), ('##ck', 1), ('##ice', 1), ('regis', 1), ('##tee', 1), ('debt', 1), ('eu', 1), ('approach', 1), ('matter', 1), ('debts', 1), ('elsa', 1), ('lo', 1), ('jerk', 1), ('uh', 1), ('seal', 1), ('##eo', 1), ('polar', 1), ('tanks', 1), ('3', 1), ('sp', 1), ('pattern', 1), ('punish', 1), ('blow', 1), ('##rio', 1), ('doesn', 1), ('lieutenant', 1), ('mel', 1), ('corrosion', 1), ('bit', 1), ('removes', 1), ('##tiv', 1), ('##ator', 1), ('modern', 1), ('rotate', 1), ('huge', 1), ('plus', 1), ('many', 1), ('sur', 1), ('##ging', 1), ('flame', 1), ('thru', 1), ('##mming', 1), ('stone', 1), ('grants', 1), ('meat', 1), ('look', 1), ('rub', 1), ('season', 1), ('doing', 1), ('pork', 1), ('dust', 1), ('refuse', 1), ('empathy', 1), ('land', 1), ('letter', 1), ('stated', 1), ('islanders', 1), ('search', 1), ('few', 1), ('local', 1), ('results', 1), ('below', 1), ('large', 1), ('asks', 1), ('internet', 1), ('wasted', 1), ('space', 1), ('ask', 1), ('issue', 1), ('##mind', 1), ('comment', 1), ('embarrassing', 1), ('person', 1), ('guide', 1), ('deserves', 1), ('reduce', 1), ('posts', 1), ('here', 1), ('restoring', 1), ('true', 1), ('sad', 1), ('om', 1), ('##g', 1), ('been', 1), ('ile', 1), ('rat', 1), ('best', 1), ('software', 1), ('impact', 1), ('##or', 1), ('used', 1), ('screwed', 1), ('remedy', 1), ('pang', 1), ('mode', 1), ('nice', 1), ('tu', 1), ('suggested', 1), ('ic', 1), ('##lean', 1), ('method', 1), ('longer', 1), ('year', 1), ('candy', 1), ('ya', 1), ('disneyland', 1), ('importantly', 1), ('hit', 1), ('##box', 1), ('means', 1), ('pay', 1), ('blue', 1), ('boo', 1), ('##by', 1), ('misty', 1), ('lighting', 1), ('attack', 1), ('flair', 1), ('small', 1), ('picture', 1), ('next', 1), ('name', 1), ('!', 1), ('dig', 1), ('into', 1), ('greed', 1), ('##ily', 1), ('go', 1), ('him', 1), ('steal', 1), ('##har', 1), ('##ey', 1), ('##ir', 1), ('each', 1), ('share', 1), ('hoop', 1), ('##as', 1), ('##ub', 1), ('ara', 1), ('##k', 1), ('il', 1), ('reach', 1), ('turn', 1), ('rogue', 1), ('var', 1), ('wry', 1), ('##nn', 1), ('mana', 1), ('summon', 1), ('7', 1), ('onto', 1), ('board', 1), ('face', 1), ('q', 1), ('##int', 1), ('battle', 1), ('list', 1), ('answer', 1), ('wait', 1), ('games', 1), ('burning', 1), ('bolts', 1), ('thunder', 1), ('##ma', 1), ('##w', 1), ('sense', 1), ('trying', 1), ('engine', 1), ('##rano', 1), ('option', 1), ('however', 1), ('requires', 1), ('lot', 1), ('prescription', 1), ('med', 1), ('mom', 1), ('committed', 1), ('suicide', 1), ('feel', 1), ('anything', 1), ('ap', 1), ('##ath', 1), ('##etic', 1), ('complete', 1), ('opposite', 1), ('normally', 1), ('emotional', 1), ('feeling', 1), ('part', 1), ('felt', 1), ('really', 1), ('knew', 1), ('wasn', 1), ('decided', 1), ('stop', 1), ('using', 1), ('stayed', 1), ('away', 1), ('ssr', 1), ('focused', 1), ('cb', 1), ('therapeutic', 1), ('forms', 1), ('instead', 1), ('most', 1), ('likely', 1), ('guys', 1), ('nothing', 1), ('mother', 1), ('upset', 1), ('age', 1), ('generally', 1), ('butt', 1), ('surprised', 1), ('.', 1), ('una', 1), ('##ppe', 1), ('##aling', 1), ('bring', 1), ('judge', 1), ('someone', 1), ('sexual', 1), ('history', 1), ('word', 1), ('forgive', 1), ('ruins', 1), ('complaining', 1), ('hash', 1), ('worlds', 1), ('##ction', 1), ('wing', 1), ('berry', 1), ('rai', 1), ('looking', 1), ('saving', 1), ('accident', 1), ('soft', 1), ('reset', 1), ('either', 1), ('codes', 1), ('both', 1), ('##ncies', 1), ('ready', 1), ('bulb', 1), ('##asa', 1), ('##ur', 1), ('ha', 1), ('5', 1), ('nest', 1), ('##ball', 1), ('going', 1), ('online', 1), ('##45', 1), ('##iv', 1), ('elect', 1), ('##rik', 1), ('pp', 1), ('hello', 1), ('dreams', 1), ('private', 1), ('networks', 1), ('civil', 1), ('##ity', 1), ('debates', 1), ('rules', 1), ('candidates', 1), ('agree', 1), ('worry', 1), ('interest', 1), ('theirs', 1), ('ratings', 1), ('concern', 1), ('views', 1), ('fairness', 1), ('being', 1), ('success', 1), ('less', 1), ('fortunate', 1), ('hard', 1), ('achieve', 1), ('wealth', 1), ('##oning', 1), ('leave', 1), ('empty', 1), ('bank', 1), ('balls', 1), ('item', 1), ('jo', 1), ('ye', 1), ('gems', 1), ('pg', 1), ('shiny', 1), ('assuming', 1), ('thread', 1), ('taking', 1), ('requests', 1), ('##ager', 1), ('spoiled', 1), ('ran', 1), ('he', 1), ('worked', 1), ('approved', 1), ('union', 1), ('none', 1), ('together', 1), ('girls', 1), ('tried', 1), ('vs', 1), ('jon', 1), ('fell', 1), ('went', 1), ('ancient', 1), ('rome', 1), ('yep', 1), ('devout', 1), ('pasta', 1), ('##od', 1), ('##age', 1), ('save', 1), ('manor', 1), ('projectile', 1), ('lion', 1), ('bone', 1), ('us', 1), ('##our', 1), ('##hs', 1), ('ll', 1), ('reaction', 1), ('bar', 1), ('keep', 1), ('eye', 1), ('bottom', 1), ('must', 1), ('feed', 1), ('cp', 1), ('120', 1), ('grams', 1), ('##ny', 1), ('##chy', 1), ('##ulu', 1), ('##ci', 1), ('##my', 1), ('lucky', 1), ('star', 1), ('puppet', 1), ('leaders', 1), ('sorry', 1), ('##ef', 1), ('##cie', 1), ('##nt', 1), ('filing', 1), ('cabinet', 1), ('ever', 1), ('seen', 1), ('damn', 1), ('thank', 1), ('widening', 1), ('textures', 1), ('already', 1), ('tile', 1), ('system', 1), ('mon', 1), ('might', 1), ('pm', 1), ('same', 1), ('happen', 1), ('gen', 1), ('##gar', 1), ('1', 1), ('short', 1), ('added', 1), ('able', 1), ('depends', 1), ('growl', 1), ('taken', 1), ('arguments', 1), ('postal', 1), ('office', 1), ('##apa', 1), ('val', 1), ('##ky', 1), ('##rus', 1), ('melody', 1), ('awakening', 1), ('dragon', 1), ('lust', 1), ('##ister', 1), ('bind', 1), ('otherwise', 1), ('##eye', 1), ('core', 1), ('ps', 1), ('##frame', 1), ('dh', 1), ('##eros', 1), ('dante', 1), ('left', 1), ('heraldic', 1), ('gaga', 1), ('##gas', 1), ('hey', 1), ('pick', 1), ('willing', 1), ('sell', 1), ('regards', 1), ('selling', 1), ('offer', 1), ('reasonable', 1), ('talk', 1), ('easier', 1), ('discuss', 1), ('prices', 1), ('rather', 1), ('ex', 1), ('children', 1), ('notorious', 1), ('##eni', 1), ('##ja', 1), ('pro', 1), ('##tea', 1), ('311', 1), ('fire', 1), ('technically', 1), ('due', 1), ('calm', 1), ('serene', 1), ('grace', 1), ('baton', 1), ('pass', 1), ('nasty', 1), ('plot', 1), ('dr', 1), ('##il', 1), ('##r', 1), ('jolly', 1), ('mold', 1), ('breaker', 1), ('x', 1), ('rapid', 1), ('sheer', 1), ('force', 1), ('##7', 1), ('dd', 1)]\n"
     ]
    }
   ],
   "source": [
    "keyword = sorted(masked_token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(keyword)\n",
    "\n",
    "masked_words = [kw for kw, freq in keyword]\n",
    "# print(masked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce8644d-d7e0-4bc8-8f5d-895735a7c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./nn_masked_words.txt\", \"w\") as f:\n",
    "    for word in masked_words:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25661661-fce1-44f5-b25d-3e10ae7e3333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
