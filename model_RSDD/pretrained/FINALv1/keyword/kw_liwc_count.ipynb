{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "112\n",
      "3000\n",
      "26526\n",
      "22966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /lustre-\n",
      "[nltk_data]     home/gpu/home/users/jakrapop.a/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to /lustre-\n",
      "[nltk_data]     home/gpu/home/users/jakrapop.a/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "##### do NOT remove STOPWORD before LIWC #####\n",
    "##### do NOT do lemma / stem before LIWC #####\n",
    "\n",
    "import liwc\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import csv\n",
    "import statistics\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "logodds_path = \"../01-logodds-topbot1500-R1-nostops.txt\"\n",
    "tfidf_path   = \"../02-tfidf-depcon3000-R1-nostops.txt\"\n",
    "lexicon_path = \"../03-depression-lexicon.txt\"\n",
    "sumatt_path  = \"../04-top-sum-attention-3000.txt\"\n",
    "topatt_path  = \"../topatt_masked_words_train.txt\"\n",
    "nn_path      = \"../nn_masked_words_train.txt\"\n",
    "\n",
    "\n",
    "with open(logodds_path) as f:\n",
    "    logodds = f.readlines()\n",
    "    logodds = [ word[:-1] for word in logodds]\n",
    "    print(len(logodds))\n",
    "    # print(logodds)\n",
    "\n",
    "    # # tokenize the lexicon and keep the input_ids of each word together in a list \n",
    "    # keyword_input_ids = [ torch.tensor((tokenizer(lex).input_ids)[1:-1]) for lex in lexicon ]\n",
    "    # print(len(keyword_input_ids))\n",
    "    # print(keyword_input_ids)\n",
    "    # keyword_list = keyword_input_ids\n",
    "    # assert keyword_list is not None\n",
    "    \n",
    "with open(tfidf_path) as f:\n",
    "    tfidf = f.readlines()\n",
    "    tfidf = [ word[:-1] for word in tfidf]\n",
    "    print(len(tfidf))\n",
    "    # print(tfidf)\n",
    "    \n",
    "with open(lexicon_path) as f:\n",
    "    lexicon = f.readlines()\n",
    "    lexicon = [ word[:-1] for word in lexicon]\n",
    "    print(len(lexicon))\n",
    "    # print(lexicon)\n",
    "    \n",
    "with open(sumatt_path) as f:\n",
    "    sumatt = f.readlines()\n",
    "    sumatt = [ word[:-1] for word in sumatt]\n",
    "    print(len(sumatt))\n",
    "    # print(sumatt)\n",
    "    \n",
    "with open(topatt_path) as f:\n",
    "    topatt = f.readlines()\n",
    "    topatt = [ word[:-1] for word in topatt]\n",
    "    print(len(topatt))\n",
    "    \n",
    "with open(nn_path) as f:\n",
    "    nn = f.readlines()\n",
    "    nn = [ word[:-1] for word in nn]\n",
    "    print(len(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772\n",
      "21754\n"
     ]
    }
   ],
   "source": [
    "sumatt_tokens = [word for word in topatt if '#' in word]\n",
    "sumatt_words = [word for word in topatt if '#' not in word]\n",
    "print(len(sumatt_tokens))\n",
    "print(len(sumatt_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'funct': [], 'pronoun': [], 'ppron': [], 'i': [], 'we': [], 'you': [], 'shehe': [], 'they': [], 'ipron': [], 'article': [], 'verb': [], 'auxverb': [], 'past': [], 'present': [], 'future': [], 'adverb': [], 'preps': [], 'conj': [], 'negate': [], 'quant': [], 'number': [], 'swear': [], 'social': [], 'family': [], 'friend': [], 'humans': [], 'affect': [], 'posemo': [], 'negemo': [], 'anx': [], 'anger': [], 'sad': [], 'cogmech': [], 'insight': [], 'cause': [], 'discrep': [], 'tentat': [], 'certain': [], 'inhib': [], 'incl': [], 'excl': [], 'percept': [], 'see': [], 'hear': [], 'feel': [], 'bio': [], 'body': [], 'health': [], 'sexual': [], 'ingest': [], 'relativ': [], 'motion': [], 'space': [], 'time': [], 'work': [], 'achieve': [], 'leisure': [], 'home': [], 'money': [], 'relig': [], 'death': [], 'assent': [], 'nonfl': [], 'filler': []}\n"
     ]
    }
   ],
   "source": [
    "parse, category_names = liwc.load_token_parser('./LIWC2007_English_copy.dic')\n",
    "\n",
    "#### Append LIWC count of each User in to each category list \n",
    "liwc_dict = {}\n",
    "for cat in category_names:\n",
    "    liwc_dict[cat] = []\n",
    "print(liwc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funct 410\n",
      "conj 26\n",
      "cogmech 1162\n",
      "incl 27\n",
      "pronoun 65\n",
      "ppron 35\n",
      "i 7\n",
      "excl 22\n",
      "article 4\n",
      "you 8\n",
      "social 623\n",
      "ipron 30\n",
      "adverb 69\n",
      "leisure 399\n",
      "verb 259\n",
      "auxverb 35\n",
      "present 116\n",
      "preps 63\n",
      "affect 1454\n",
      "posemo 702\n",
      "assent 21\n",
      "negemo 740\n",
      "anger 295\n",
      "bio 627\n",
      "sexual 83\n",
      "swear 36\n",
      "cause 220\n",
      "they 5\n",
      "number 80\n",
      "time 389\n",
      "relativ 1130\n",
      "humans 68\n",
      "nonfl 9\n",
      "shehe 9\n",
      "past 127\n",
      "tentat 164\n",
      "work 695\n",
      "achieve 454\n",
      "space 415\n",
      "relig 239\n",
      "health 224\n",
      "percept 465\n",
      "see 154\n",
      "quant 108\n",
      "negate 13\n",
      "hear 103\n",
      "body 217\n",
      "we 6\n",
      "motion 310\n",
      "future 10\n",
      "death 86\n",
      "family 63\n",
      "friend 53\n",
      "ingest 142\n",
      "feel 133\n",
      "certain 110\n",
      "money 276\n",
      "insight 395\n",
      "home 139\n",
      "discrep 64\n",
      "inhib 228\n",
      "sad 149\n",
      "anx 152\n",
      "filler 2\n",
      "{'funct': 410, 'pronoun': 65, 'ppron': 35, 'i': 7, 'we': 6, 'you': 8, 'shehe': 9, 'they': 5, 'ipron': 30, 'article': 4, 'verb': 259, 'auxverb': 35, 'past': 127, 'present': 116, 'future': 10, 'adverb': 69, 'preps': 63, 'conj': 26, 'negate': 13, 'quant': 108, 'number': 80, 'swear': 36, 'social': 623, 'family': 63, 'friend': 53, 'humans': 68, 'affect': 1454, 'posemo': 702, 'negemo': 740, 'anx': 152, 'anger': 295, 'sad': 149, 'cogmech': 1162, 'insight': 395, 'cause': 220, 'discrep': 64, 'tentat': 164, 'certain': 110, 'inhib': 228, 'incl': 27, 'excl': 22, 'percept': 465, 'see': 154, 'hear': 103, 'feel': 133, 'bio': 627, 'body': 217, 'health': 224, 'sexual': 83, 'ingest': 142, 'relativ': 1130, 'motion': 310, 'space': 415, 'time': 389, 'work': 695, 'achieve': 454, 'leisure': 399, 'home': 139, 'money': 276, 'relig': 239, 'death': 86, 'assent': 21, 'nonfl': 9, 'filler': 2}\n",
      "410\n",
      "65\n",
      "35\n",
      "7\n",
      "6\n",
      "8\n",
      "9\n",
      "5\n",
      "30\n",
      "4\n",
      "259\n",
      "35\n",
      "127\n",
      "116\n",
      "10\n",
      "69\n",
      "63\n",
      "26\n",
      "13\n",
      "108\n",
      "80\n",
      "36\n",
      "623\n",
      "63\n",
      "53\n",
      "68\n",
      "1454\n",
      "702\n",
      "740\n",
      "152\n",
      "295\n",
      "149\n",
      "1162\n",
      "395\n",
      "220\n",
      "64\n",
      "164\n",
      "110\n",
      "228\n",
      "27\n",
      "22\n",
      "465\n",
      "154\n",
      "103\n",
      "133\n",
      "627\n",
      "217\n",
      "224\n",
      "83\n",
      "142\n",
      "1130\n",
      "310\n",
      "415\n",
      "389\n",
      "695\n",
      "454\n",
      "399\n",
      "139\n",
      "276\n",
      "239\n",
      "86\n",
      "21\n",
      "9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "liwc_counts = Counter(category for token in sumatt_words for category in parse(token))\n",
    "\n",
    "for key, value in liwc_counts.items():\n",
    "    print(key, value)\n",
    "    liwc_dict[key] = value\n",
    "        \n",
    "print(liwc_dict)\n",
    "\n",
    "for key, value in liwc_dict.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count total number of tokens that liwc has seen\n",
    "# total_tokens = 0\n",
    "\n",
    "# for text in text_all:\n",
    "#     total_tokens += len(word_tokenize(text))\n",
    "    \n",
    "# print('total_tokens : ', total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54ebe46dc67c0b0016e368835037c988a8dce633f341e79a61a84613b212514"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
