{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90a8ced4-5735-46de-9aba-7fbcf02490cd",
   "metadata": {},
   "source": [
    "# KE-MLM - Log-odd-ratio\n",
    "\n",
    "- use domain dataset as background corpus\n",
    "- use depression as corpus_i\n",
    "- collect top and bottom 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab43e2-c016-44db-8de9-0c834c1734d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_punctuations   = True\n",
    "rm_stopwords      = True\n",
    "save_top_words    = 1500\n",
    "\n",
    "lower_case        = False # already DONE\n",
    "tokenizer         = None # use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5de71f-ba17-4241-8c11-03837c1d6afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# LogOddsRatio Class\n",
    "# \n",
    "# A class for computing Log-odds-ratio with informative Dirichlet priors\n",
    "#\n",
    "# See http://languagelog.ldc.upenn.edu/myl/Monroe.pdf for more detail\n",
    "# \n",
    "#####################################################################\n",
    "\n",
    "__author__ = \"Kornraphop Kawintiranon\"\n",
    "__email__ = \"kornraphop.k@gmail.com\"\n",
    "\n",
    "import math\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import json, pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizerFast, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "# from src.models  import *\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e319f19e-dc0a-4d46-9899-c87318839213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "\n",
    "def parallel_tokenize(corpus, tokenizer=None, n_jobs=-1):\n",
    "    if tokenizer == None:\n",
    "        tokenizer = NLTKWordTokenizer()\n",
    "    if n_jobs < 0:\n",
    "        n_jobs = multiprocessing.cpu_count() - 1\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        corpus_tokenized = list(\n",
    "            tqdm.tqdm(executor.map(tokenizer.tokenize, corpus, chunksize=200), total=len(corpus), desc='Tokenizing')\n",
    "        )\n",
    "    return corpus_tokenized\n",
    "\n",
    "def remove_stopwords(corpus, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        \n",
    "        # print(words[:100])\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # print(words[:100])\n",
    "        # asdfasfasdf\n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "\n",
    "def remove_punctuations(corpus):\n",
    "    punctuations = string.punctuation\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        # remove single punctuations\n",
    "        words = [w for w in words if not w in punctuations]\n",
    "        words = [re.sub(r\"\"\"[()#[\\]#*+\\-/:;<=>@[\\]^_`{|}~\"\\\\.?!$%&]\"\"\", \"\", w) for w in words]      \n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "    \n",
    "def decontract(corpus):\n",
    "    processed_corpus = []\n",
    "    for phrase in tqdm.tqdm(corpus, desc=\"Decontracting\"):\n",
    "        phrase = re.sub(r\"â€™\", \"\\'\", phrase)\n",
    "\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "        processed_corpus.append(phrase)\n",
    "    return processed_corpus\n",
    "\n",
    "def get_word_counts(corpus):\n",
    "    # Initializing Dictionary\n",
    "    d = {}\n",
    "\n",
    "    # Counting number of times each word comes up in list of words (in dictionary)\n",
    "    for words in tqdm.tqdm(corpus, desc=\"Word Counting\"):\n",
    "        for w in words:\n",
    "            d[w] = d.get(w, 0) + 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c22b4a-1ce0-4b0d-950d-a46da5f55f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10024851-a07b-41d3-861a-fe8cbcb6e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogOddsRatio:\n",
    "    \"\"\"\n",
    "    Log-odds-ratio with informative Dirichlet priors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_i, corpus_j, background_corpus=None, lower_case=True, rm_stopwords=True, rm_punctuations=True, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Create a class object and prepare word counts for log-odds-ratio computation\n",
    "        Args:\n",
    "            corpus_i:        A list of documents, each contains a string\n",
    "            corpus_j:        A list of documents, each contains a string\n",
    "            background_corpus (default = None): If None, it will be assigned to a concatenation of `corpus_i` and `corpus_j`\n",
    "            rm_stopwords:    Whether remove stopwords in preprocessing step\n",
    "            tokenizer:       To specify a specific tokenizer for tokenization step\n",
    "        \"\"\"\n",
    "\n",
    "        def preprocessing(corpus):\n",
    "            if lower_case:\n",
    "                print(\"lowercasing\")\n",
    "                corpus = [text.lower() for text in corpus]\n",
    "            corpus = decontract(corpus)\n",
    "            tokenized_corpus = parallel_tokenize(corpus, tokenizer)\n",
    "\n",
    "            if rm_stopwords:\n",
    "                print(\"removing stopwords\")\n",
    "                tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "\n",
    "            if rm_punctuations:\n",
    "                print(\"removing punctuation\")\n",
    "                tokenized_corpus = remove_punctuations(tokenized_corpus)\n",
    "\n",
    "            print(tokenized_corpus[0][:500])\n",
    "\n",
    "            return tokenized_corpus\n",
    "\n",
    "        # Convert a list of string into a list of lists of words\n",
    "        logger.info(\"Preprocessing corpus-i\")\n",
    "        corpus_i = preprocessing(corpus_i)\n",
    "        logger.info(\"Preprocessing corpus-j\")\n",
    "        corpus_j = preprocessing(corpus_j)\n",
    "        if background_corpus != None:\n",
    "            logger.info(\"Preprocessing corpus-background\")\n",
    "            background_corpus = preprocessing(background_corpus)\n",
    "        \n",
    "        # Compute word counts of every words on each corpus separately\n",
    "        logger.info(\"Getting word counts from corpus-i\")\n",
    "        self.y_i = get_word_counts(corpus_i)\n",
    "        logger.info(\"Getting word counts from corpus-j\")\n",
    "        self.y_j = get_word_counts(corpus_j)\n",
    "        logger.info(\"Getting word counts from corpus-background\")\n",
    "        if background_corpus:\n",
    "            self.alpha = get_word_counts(background_corpus)\n",
    "        else:\n",
    "            # Combine words and sum their counts of corpus i and j in case no specified background corpus\n",
    "            self.alpha = {k: self.y_i.get(k, 0) + self.y_j.get(k, 0) for k in set(self.y_i) | set(self.y_j)}\n",
    "\n",
    "        # Sort dicts\n",
    "        logger.debug(\"Start sorting and backing up to files\")\n",
    "        self.y_i = {k: v for k, v in sorted(self.y_i.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.y_j = {k: v for k, v in sorted(self.y_j.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.alpha = {k: v for k, v in sorted(self.alpha.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        # Write to files as backup\n",
    "        with open(\"vocabs_i.txt\", \"w\") as f:\n",
    "            for k, v in self.y_i.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"vocabs_j.txt\", \"w\") as f:\n",
    "            for k, v in self.y_j.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"vocabs_alpha.txt\", \"w\") as f:\n",
    "            for k, v in self.alpha.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "\n",
    "        # Initialize necessary variables\n",
    "        self.delta = None\n",
    "        self.sigma_2 = None\n",
    "        self.z_scores = None\n",
    "\n",
    "        # Compute\n",
    "        logger.info(\"Start computing delta\")\n",
    "        self._compute_delta()\n",
    "        logger.info(\"Start computing sigma^2\")\n",
    "        self._compute_sigma_2()\n",
    "        logger.info(\"Start computing Z-score\")\n",
    "        self._compute_z_scores()\n",
    "\n",
    "        # Sort dicts\n",
    "        logger.debug(\"Start sorting and backing up to files\")\n",
    "        self.delta = {k: v for k, v in sorted(self.delta.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.sigma_2 = {k: v for k, v in sorted(self.sigma_2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.z_scores = {k: v for k, v in sorted(self.z_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        # Write to files as backup\n",
    "        with open(\"delta.txt\", \"w\") as f:\n",
    "            for k, v in self.delta.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"sigma_2.txt\", \"w\") as f:\n",
    "            for k, v in self.sigma_2.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"z_scores.txt\", \"w\") as f:\n",
    "            for k, v in self.z_scores.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "\n",
    "\n",
    "    def _compute_delta(self):\n",
    "            \"\"\" The usage difference for word w among two corpora i and j\n",
    "            \"\"\"\n",
    "            self.delta = dict()\n",
    "            n_i = sum(self.y_i.values())\n",
    "            n_j = sum(self.y_j.values())\n",
    "            alpha_zero = sum(self.alpha.values())\n",
    "            logger.debug(f\"Size of corpus-i: {n_i}\")\n",
    "            logger.debug(f\"Size of corpus-j: {n_j}\")\n",
    "            logger.debug(f\"Size of background corpus: {alpha_zero}\")\n",
    "\n",
    "            try:\n",
    "                for w in set(self.y_i) | set(self.y_j): # iterate through all words among two corpora\n",
    "\n",
    "                    # print(self.y_i.get(w, 0))\n",
    "                    # print(self.alpha.get(w, 0))\n",
    "                    # print(n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0))\n",
    "\n",
    "                    first_top    = self.y_i.get(w, 0) + self.alpha.get(w, 0)\n",
    "                    first_bottom = n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0)\n",
    "\n",
    "                    second_top    = self.y_j.get(w, 0) + self.alpha.get(w, 0)\n",
    "                    second_bottom = n_j + alpha_zero - self.y_j.get(w, 0) - self.alpha.get(w, 0)\n",
    "\n",
    "\n",
    "                    if first_bottom == 0 and second_bottom == 0:\n",
    "                        first_log  = 0\n",
    "                        second_log = 0\n",
    "\n",
    "                    if first_bottom == 0 and second_bottom != 0:\n",
    "                        first_log  = 0\n",
    "                        second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                    if second_bottom == 0:\n",
    "                        first_log = math.log10( first_top / first_bottom )\n",
    "                        second_log = 0\n",
    "\n",
    "\n",
    "                    if first_bottom != 0 and second_bottom != 0:\n",
    "                        if (first_top / first_bottom) == 0 and (second_top / second_bottom) != 0:\n",
    "                            first_log = 0\n",
    "                            second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                        if (first_top / first_bottom) != 0 and (second_top / second_bottom) == 0:\n",
    "                            first_log  = math.log10( first_top  / first_bottom )\n",
    "                            second_log = 0\n",
    "\n",
    "                        if (first_top / first_bottom) != 0 and (second_top / second_bottom) != 0:\n",
    "                            first_log  = math.log10( first_top  / first_bottom )\n",
    "                            second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                    self.delta[w] = first_log - second_log\n",
    "\n",
    "            except ValueError as e:\n",
    "                logger.debug(f\"Y-i of the word {w}:\", self.y_i.get(w, 0))\n",
    "                logger.debug(f\"alpha of the word {w}:\", self.alpha.get(w, 0))\n",
    "                logger.debug(f\"value:\", (self.y_i.get(w, 0) + self.alpha.get(w, 0)) /\n",
    "                      (n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0)))\n",
    "                raise e\n",
    "\n",
    "    def _compute_sigma_2(self):\n",
    "        \"\"\" Compute estimated values of sigma squared\n",
    "        \"\"\"\n",
    "        self.sigma_2 = dict()\n",
    "        for w in self.delta:\n",
    "            if (self.y_i.get(w, 0) + self.alpha.get(w, 0)) == 0 or (self.y_j.get(w, 0) + self.alpha.get(w, 0)) == 0:\n",
    "                self.sigma_2[w] = 0\n",
    "            else:\n",
    "                self.sigma_2[w] = (1 / (self.y_i.get(w, 0) + self.alpha.get(w, 0))) + (1 / (self.y_j.get(w, 0) + self.alpha.get(w, 0)))\n",
    "\n",
    "    def _compute_z_scores(self):\n",
    "        self.z_scores = dict()\n",
    "        for w in self.delta:\n",
    "            if self.sigma_2.get(w, 0) == 0:\n",
    "                # score 0 is in the middle so it will not show up in top or bottom which is what we want!\n",
    "                self.z_scores[w] = 0\n",
    "            else:\n",
    "                self.z_scores[w] = self.delta.get(w, 0) / math.sqrt(self.sigma_2.get(w, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29563ec0-ac5a-42c2-89d5-4b6cb75bbe4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WITH BG CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aaf9997-b2b4-42ac-92e9-310c187d337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_corpus = pickle.load(open(\"../data/domain/domain_corpus_traindepcon_ratio10.pkl\", \"rb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b46ea-45ab-49cf-a00b-942edd2fd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ratio = 1\n",
    "\n",
    "all_train_depression_text = pickle.load(open(f\"../data/classi/classi_corpus_traindep_ratio{classi_ratio}.pkl\", \"rb\"))\n",
    "all_train_control_text    = pickle.load(open(f\"../data/classi/classi_corpus_traincon_ratio{classi_ratio}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f66f0-c22e-4000-93b4-f1df2f642062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRESSION = i\n",
    "corpus_i  = all_train_depression_text\n",
    "corpus_j  = all_train_control_text\n",
    "background_corpus = background_corpus\n",
    "\n",
    "log_odds_ratio = LogOddsRatio(corpus_i          = corpus_i,\n",
    "                              corpus_j          = corpus_j, \n",
    "                              background_corpus = background_corpus,\n",
    "                              lower_case        = lower_case, \n",
    "                              rm_stopwords      = rm_stopwords, \n",
    "                              rm_punctuations   = rm_punctuations, \n",
    "                              tokenizer         = None)\n",
    "\n",
    "# Save top words into a file\n",
    "if save_top_words != None and save_top_words > 0:\n",
    "    if save_top_words > len(log_odds_ratio.z_scores):\n",
    "        raise ValueError(\"--save_top_words must be less than or equal to vocab size\")\n",
    "\n",
    "    logger.info(f\"Saving top and bottom {save_top_words} words ranked by Z-score\")\n",
    "    tops    = list(log_odds_ratio.z_scores.keys())[:save_top_words]\n",
    "    bottoms = list(log_odds_ratio.z_scores.keys())[-save_top_words:]\n",
    "\n",
    "    with open(f\"./logodds-topbot{save_top_words}-R{classi_ratio}-nostops-NEW.txt\", \"w\") as f:\n",
    "        for word in tops:\n",
    "            f.write(word + \"\\n\")\n",
    "        for word in bottoms:\n",
    "            f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c37712-f20d-414a-b834-18307c028664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONTROL = i\n",
    "# corpus_i  = all_train_control_text\n",
    "# corpus_j  = all_train_depression_text\n",
    "# corpus_bg = background_corpus\n",
    "\n",
    "# log_odds_ratio = LogOddsRatio(corpus_i, corpus_j, corpus_bg=corpus_bg, tokenizer=None, rm_stopwords=rm_stopwords)\n",
    "\n",
    "# # Save top words into a file\n",
    "# if save_top_words != None and save_top_words > 0:\n",
    "#     if save_top_words > len(log_odds_ratio.z_scores):\n",
    "#         raise ValueError(\"--save_top_words must be less than or equal to vocab size\")\n",
    "\n",
    "#     logger.info(f\"Saving top and bottom {save_top_words} words ranked by Z-score\")\n",
    "#     tops    = list(log_odds_ratio.z_scores.keys())[:save_top_words]\n",
    "#     bottoms = list(log_odds_ratio.z_scores.keys())[-save_top_words:]\n",
    "\n",
    "#     with open(\"./new-KE-CONbg-topbottom45-words.txt\", \"w\") as f:\n",
    "#         for word in tops:\n",
    "#             f.write(word + \"\\n\")\n",
    "#         for word in bottoms:\n",
    "#             f.write(word + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5834f4bf-14d3-44df-aa8e-8f15210311ec",
   "metadata": {},
   "source": [
    "# WITHOUT BG CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2676edac-bc9a-4d9c-8fa6-22a3e214d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_depression_text = pickle.load(open(\"../../data/traindeptext_concat3m_ratio12.pkl\", \"rb\"))\n",
    "# all_train_control_text    = pickle.load(open(\"../../data/traincontext_concat3m_ratio12.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095edf0c-998f-4661-8b1f-1508282b50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEPRESSION = i\n",
    "# corpus_i = all_train_depression_text\n",
    "# corpus_j = all_train_control_text\n",
    "\n",
    "# log_odds_ratio = LogOddsRatio(corpus_i, corpus_j, corpus_bg=None, tokenizer=None, rm_stopwords=rm_stopwords)\n",
    "\n",
    "# # Save top words into a file\n",
    "# if save_top_words != None and save_top_words > 0:\n",
    "#     if save_top_words > len(log_odds_ratio.z_scores):\n",
    "#         raise ValueError(\"--save_top_words must be less than or equal to vocab size\")\n",
    "\n",
    "#     logger.info(f\"Saving top and bottom {save_top_words} words ranked by Z-score\")\n",
    "#     tops    = list(log_odds_ratio.z_scores.keys())[:save_top_words]\n",
    "#     bottoms = list(log_odds_ratio.z_scores.keys())[-save_top_words:]\n",
    "\n",
    "#     with open(\"./new-KE-DEP-topbottom45-words.txt\", \"w\") as f:\n",
    "#         for word in tops:\n",
    "#             f.write(word + \"\\n\")\n",
    "#         for word in bottoms:\n",
    "#             f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6910cc00-b75d-4b5f-9a59-c16d1cf11aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONTROL = i\n",
    "# corpus_i = all_train_control_text\n",
    "# corpus_j = all_train_depression_text\n",
    "\n",
    "# log_odds_ratio = LogOddsRatio(corpus_i, corpus_j, corpus_bg=None, tokenizer=None, rm_stopwords=rm_stopwords)\n",
    "\n",
    "# # Save top words into a file\n",
    "# if save_top_words != None and save_top_words > 0:\n",
    "#     if save_top_words > len(log_odds_ratio.z_scores):\n",
    "#         raise ValueError(\"--save_top_words must be less than or equal to vocab size\")\n",
    "\n",
    "#     logger.info(f\"Saving top and bottom {save_top_words} words ranked by Z-score\")\n",
    "#     tops    = list(log_odds_ratio.z_scores.keys())[:save_top_words]\n",
    "#     bottoms = list(log_odds_ratio.z_scores.keys())[-save_top_words:]\n",
    "\n",
    "#     with open(\"./new-KE-CON-topbottom45-words.txt\", \"w\") as f:\n",
    "#         for word in tops:\n",
    "#             f.write(word + \"\\n\")\n",
    "#         for word in bottoms:\n",
    "#             f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09d75ed-8f5c-4098-a482-54d51168e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./new-KE-DEP-topbottom45-words.txt\") as f:\n",
    "#     DEP = f.readlines()\n",
    "# DEP = [i[:-1] for i in d1_all]\n",
    "# print(DEP)\n",
    "\n",
    "# with open(\"./new-KE-DEPbg-topbottom45-words.txt\") as f:\n",
    "#     DEPbg = f.readlines()\n",
    "# DEPbg = [i[:-1] for i in d1_all]\n",
    "# print(DEPbg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
