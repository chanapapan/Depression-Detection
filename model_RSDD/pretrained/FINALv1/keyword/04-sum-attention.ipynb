{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASKER KEYWORD - Attention\n",
    "\n",
    "### Top words by highest sum of attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load model classi ratio 1\n",
    "- dataset ratio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "from src.traineval  import *\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_from = '../save/BASE-classiCEr1/best-model-1500.tar'\n",
    "\n",
    "checkpoint           = 'bert-base-uncased'\n",
    "\n",
    "training_obj       = 'classiCE'\n",
    "masking_method     = None\n",
    "keyword_path       = None\n",
    "\n",
    "classifier_p_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from  ../save/BASE-classiCEr1/best-model-1500.tar\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2 #CE\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels).to(device)\n",
    "model.classifier.dropout = nn.Dropout(p = classifier_p_dropout, inplace = False)\n",
    "\n",
    "\n",
    "print(\"Load from \", load_model_from)\n",
    "checkpoint = torch.load(load_model_from)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(model.load_state_dict(checkpoint['model_state_dict']))\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint   = 'bert-base-uncased'\n",
    "tokenizer    =  AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open(f'../data/classi/classichunk-R1-train-ds.pkl', \"rb\"))\n",
    "\n",
    "for sample in train_dataset:\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['word_ids'])\n",
    "    print(sample['attention_mask'])\n",
    "    print(sample['orig_text'])\n",
    "    print(sample['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL CODE !!!\n",
    "def get_attention_keyword(dataset, model, tokenizer, device, num_kw):\n",
    "\n",
    "    loader = DataLoader(dataset, shuffle=False, batch_size = 16)\n",
    "\n",
    "    SPECIAL_TOKENS = tokenizer.all_special_ids\n",
    "    vocab_size = len(tokenizer)\n",
    "\n",
    "    attn_score = torch.zeros(vocab_size)\n",
    "    attn_freq  = torch.zeros(vocab_size)\n",
    "\n",
    "    for idx , batch in enumerate(loader):\n",
    "        \n",
    "        sys.stdout.write(str(idx))\n",
    "        \n",
    "        tokens   = batch['input_ids'].to(device)\n",
    "        word_ids = batch['word_ids']\n",
    "        labels   = batch['labels'].cpu()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output    = model(tokens, output_attentions=True) # (batch_size, num_heads, sequence_length, sequence_length)           \n",
    "            attention = output.attentions[-1] # get attention of last layer (batch_size, num_heads, sequence_length, sequence_length)\n",
    "        \n",
    "        pred = torch.argmax(torch.softmax(output.logits.detach(), dim = 1), dim = 1).detach().cpu()\n",
    "        \n",
    "        correct_idx = (labels == pred).nonzero(as_tuple=True)[0].detach().cpu()\n",
    "        # print(correct_idx)\n",
    "        \n",
    "        correct_attention = torch.index_select(attention.clone().detach().cpu(), dim = 0 , index = correct_idx)\n",
    "        # print(correct_attention.shape)\n",
    "        \n",
    "        attention = correct_attention.sum(dim = 1) # sum over attention heads (batch_size, sequence_length, sequence_length)\n",
    "        \n",
    "        for  i in range(attention.size(0)):  # for each sample in batch\n",
    "            for j in range(attention.size(-1)):  # max_len\n",
    "                token = tokens[i][j].item()\n",
    "                \n",
    "                if token in SPECIAL_TOKENS:  # skip special token\n",
    "                    continue\n",
    "\n",
    "                score = attention[i][0][j]  # 1st token = CLS token\n",
    "\n",
    "                attn_score[token] += score.item()\n",
    "                attn_freq[token] += 1\n",
    "\n",
    "    for tok in range(vocab_size):\n",
    "        \n",
    "        if attn_freq[tok] < 10 : # if freq less than 10 REMOVE from the list !\n",
    "            attn_score[tok] = 0\n",
    "            \n",
    "        else:\n",
    "            attn_score[tok] /= attn_freq[tok]  # normalize by frequency\n",
    "\n",
    "    keyword = attn_score.argsort(descending=True)[:num_kw].tolist()\n",
    "\n",
    "    return keyword, attn_score, attn_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289"
     ]
    }
   ],
   "source": [
    "keyword, attn_score, attn_freq = get_attention_keyword(train_dataset, model, tokenizer, device, num_kw = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "keywords = [tokenizer.decode([word]) for word in keyword]\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./top-sum-attention-3000.txt\", \"w\") as f:\n",
    "    for word in keywords:\n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54ebe46dc67c0b0016e368835037c988a8dce633f341e79a61a84613b212514"
  },
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
