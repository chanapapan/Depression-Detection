{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd8238c-613e-40bb-8ec9-e27583fd106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "112\n",
      "3000\n",
      "26526\n",
      "22966\n"
     ]
    }
   ],
   "source": [
    "logodds_path = \"./01-logodds-topbot1500-R1-nostops.txt\"\n",
    "tfidf_path   = \"./02-tfidf-depcon3000-R1-nostops.txt\"\n",
    "lexicon_path = \"./03-depression-lexicon.txt\"\n",
    "sumatt_path  = \"./04-top-sum-attention-3000.txt\"\n",
    "topatt_path  = \"./topatt_masked_words_train.txt\"\n",
    "nn_path      = \"./nn_masked_words_train.txt\"\n",
    "\n",
    "\n",
    "with open(logodds_path) as f:\n",
    "    logodds = f.readlines()\n",
    "    logodds = [ word[:-1] for word in logodds]\n",
    "    print(len(logodds))\n",
    "    # print(logodds)\n",
    "\n",
    "    # # tokenize the lexicon and keep the input_ids of each word together in a list \n",
    "    # keyword_input_ids = [ torch.tensor((tokenizer(lex).input_ids)[1:-1]) for lex in lexicon ]\n",
    "    # print(len(keyword_input_ids))\n",
    "    # print(keyword_input_ids)\n",
    "    # keyword_list = keyword_input_ids\n",
    "    # assert keyword_list is not None\n",
    "    \n",
    "with open(tfidf_path) as f:\n",
    "    tfidf = f.readlines()\n",
    "    tfidf = [ word[:-1] for word in tfidf]\n",
    "    print(len(tfidf))\n",
    "    # print(tfidf)\n",
    "    \n",
    "with open(lexicon_path) as f:\n",
    "    lexicon = f.readlines()\n",
    "    lexicon = [ word[:-1] for word in lexicon]\n",
    "    print(len(lexicon))\n",
    "    # print(lexicon)\n",
    "    \n",
    "with open(sumatt_path) as f:\n",
    "    sumatt = f.readlines()\n",
    "    sumatt = [ word[:-1] for word in sumatt]\n",
    "    print(len(sumatt))\n",
    "    # print(sumatt)\n",
    "    \n",
    "with open(topatt_path) as f:\n",
    "    topatt = f.readlines()\n",
    "    topatt = [ word[:-1] for word in topatt]\n",
    "    print(len(topatt))\n",
    "    \n",
    "with open(nn_path) as f:\n",
    "    nn = f.readlines()\n",
    "    nn = [ word[:-1] for word in nn]\n",
    "    print(len(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38250d63-37f5-4552-9944-f113e6b76cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772\n",
      "21754\n"
     ]
    }
   ],
   "source": [
    "sumatt_tokens = [word for word in nn if '#' in word]\n",
    "sumatt_words = [word for word in nn if '#' not in word]\n",
    "print(len(sumatt_tokens))\n",
    "print(len(sumatt_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed484c21-133b-4382-b495-c99e6a8f05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PUNCT': 11, 'CCONJ': 10, 'PRON': 43, 'DET': 22, 'ADV': 608, 'NOUN': 6334, 'AUX': 15, 'SCONJ': 19, 'PROPN': 8585, 'INTJ': 78, 'X': 206, 'VERB': 3181, 'ADP': 60, 'NUM': 794, 'ADJ': 1785, 'PART': 2, 'SYM': 1}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "count_sumatt_pos_spacy = {}\n",
    "\n",
    "for word in sumatt_words:\n",
    "    sen = sp(word)\n",
    "    tag = sen[0].pos_\n",
    "    word = sen[0]\n",
    "    # print(sen[0], sen[0].pos_ ) #, sen[0].tag_)\n",
    "    \n",
    "    if tag not in count_sumatt_pos_spacy.keys():\n",
    "        count_sumatt_pos_spacy[tag] = 1\n",
    "    else : \n",
    "        count_sumatt_pos_spacy[tag] += 1\n",
    "    \n",
    "print(count_sumatt_pos_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43d1ddec-ac81-4795-9ad8-e5902b8ad0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 3, \"''\": 1, ',': 1, 'CC': 14, 'NN': 7708, 'DT': 22, 'PRP': 21, 'RB': 766, 'PRP$': 8, 'VBZ': 374, 'WDT': 6, 'VBP': 1463, 'IN': 214, 'VB': 101, 'NNS': 2990, 'CD': 833, 'JJ': 4222, 'VBD': 1140, 'VBN': 487, 'TO': 1, 'MD': 13, 'FW': 51, 'VBG': 1108, 'NNP': 30, 'WP': 8, 'RP': 8, 'JJS': 48, 'WRB': 6, 'RBR': 45, 'RBS': 3, 'POS': 1, 'JJR': 51, '$': 6, 'WP$': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "all_pos_tag = pos_tag(sumatt_words) \n",
    "\n",
    "count_sumatt_pos_nltk = {}\n",
    "\n",
    "for pair in all_pos_tag:\n",
    "    \n",
    "    # print(pair[1])\n",
    "    word = pair[0]\n",
    "    tag  = pair[1]\n",
    "    \n",
    "    if tag not in count_sumatt_pos_nltk.keys():\n",
    "        count_sumatt_pos_nltk[tag] = 1\n",
    "    else : \n",
    "        count_sumatt_pos_nltk[tag] += 1\n",
    "        \n",
    "print(count_sumatt_pos_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4762a301-ccaf-4e49-97d2-9eb102389541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /lustre-\n",
      "[nltk_data]     home/gpu/home/users/jakrapop.a/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('tagsets')\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405b7d09-97db-4150-84fe-1b69829227c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# stopwords = [word for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4cb56c-dd3e-462c-a114-920b9f32ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': 20, 'VERB': 15, 'PRON': 28, 'ADJ': 6, 'SCONJ': 6, 'DET': 18, 'ADV': 23, 'ADP': 22, 'AUX': 24, 'X': 8, 'NOUN': 1, 'CCONJ': 4, 'INTJ': 3, 'PART': 1}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "count_sumatt_pos_spacy = {}\n",
    "\n",
    "for word in stop_words:\n",
    "    sen = sp(word)\n",
    "    tag = sen[0].pos_\n",
    "    word = sen[0]\n",
    "    # print(sen[0], sen[0].pos_ ) #, sen[0].tag_)\n",
    "    \n",
    "    if tag not in count_sumatt_pos_spacy.keys():\n",
    "        count_sumatt_pos_spacy[tag] = 1\n",
    "    else : \n",
    "        count_sumatt_pos_spacy[tag] += 1\n",
    "    \n",
    "print(count_sumatt_pos_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68aa05c2-cf6e-4e06-b89b-1aead9b8c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JJ': 16, 'NN': 23, 'PRP': 16, 'VBP': 9, 'JJR': 2, 'VBZ': 6, 'IN': 28, 'NNS': 6, 'MD': 3, 'WDT': 1, 'RB': 13, 'TO': 1, 'VBD': 5, 'VBN': 3, 'VB': 8, 'PRP$': 7, 'DT': 12, 'WRB': 4, 'CC': 4, 'FW': 1, 'EX': 1, 'RP': 3, 'VBG': 3, 'WP': 3, 'RBS': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "all_pos_tag = pos_tag(stop_words) \n",
    "\n",
    "count_sumatt_pos_nltk = {}\n",
    "\n",
    "for pair in all_pos_tag:\n",
    "    \n",
    "    # print(pair[1])\n",
    "    word = pair[0]\n",
    "    tag  = pair[1]\n",
    "    \n",
    "    if tag not in count_sumatt_pos_nltk.keys():\n",
    "        count_sumatt_pos_nltk[tag] = 1\n",
    "    else : \n",
    "        count_sumatt_pos_nltk[tag] += 1\n",
    "        \n",
    "print(count_sumatt_pos_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f451c4d-f101-4c30-8ded-afe8d96412e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
