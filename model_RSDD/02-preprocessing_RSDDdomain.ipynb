{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression\n"
     ]
    }
   ],
   "source": [
    "d_class      = 'depression' # / 'control'\n",
    "\n",
    "print(d_class)\n",
    "\n",
    "##############################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE !!!!\n",
    "Convert text_only.csv to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor can REMOVE these things\n",
    "# URL\t            p.OPT.URL\n",
    "# Mention\t        p.OPT.MENTION\n",
    "# Hashtag\t        p.OPT.HASHTAG\n",
    "# Reserved Words\tp.OPT.RESERVED for twitters ex RT\n",
    "# Emoji\t            p.OPT.EMOJI\n",
    "# Smiley\t        p.OPT.SMILEY\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "input_file_name = f'data_{d_class}/training/text_only.txt'\n",
    "p.clean_file(input_file_name, p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "print(\"Success !!!\")\n",
    "\n",
    "# number of line is still equal to the original text_only file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename to pclean.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW 05/2022 Ver.\n",
    "\n",
    "def preprocess(row):\n",
    "    \n",
    "    row = re.sub(\"\\[\\]\\((.*?)\\)\", \"\", row) # remove these things that comes with Reddit [](/trixiesmug)\n",
    "    row = re.sub(\"\\\\r\\\\\\w+\",'', row) # remove these things \\r\\books\n",
    "    row = re.sub(\"r/\\w+\",'', row) # remove these things r/books\n",
    "    \n",
    "    # Unescape HTML tags\n",
    "    row = BeautifulSoup(unescape(row), 'lxml').text\n",
    "    \n",
    "    # Remove all punctuations except . ! ? $ % &\n",
    "    row = re.sub(r\"\"\"[()#[\\]#*+\\-/:;<=>@[\\]^_`{|}~\"\\\\]\"\"\", \"\", row)\n",
    "    # Remove Duplicated punctuations \n",
    "    row = re.sub(r\"\"\"(?<=[^!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~][!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~])[!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~ ]+(?<! )\"\"\", \"\", row)\n",
    "    row = re.sub(r'!{2,}', r'!', row)\n",
    "    row = re.sub(r'\\.{2,}', r'.', row)\n",
    "    row = re.sub(r'\\&{2,}', r'&', row)\n",
    "    row = re.sub(r'\\%{2,}', r'%', row)\n",
    "    row = re.sub(r'\\?{2,}', r'?', row)\n",
    "    row = re.sub(r'\\${2,}', r'$', row)\n",
    "\n",
    "    # Lowercase\n",
    "    row = ' '.join([w.lower() for w in row.split()])\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    row = ' '.join([w for w in row.split()])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32014200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_file  = f'data_{d_class}/training/pclean_0529.txt'\n",
    "output_file = f'data_{d_class}/training/finclean_0529.txt'\n",
    "\n",
    "with open(input_file) as f, open(output_file, 'w') as o:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    for line in lines:\n",
    "        # print(line)\n",
    "        cleaned = preprocess(line)\n",
    "        o.write(cleaned + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54ebe46dc67c0b0016e368835037c988a8dce633f341e79a61a84613b212514"
  },
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
