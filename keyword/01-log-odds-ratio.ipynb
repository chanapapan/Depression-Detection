{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a8ced4-5735-46de-9aba-7fbcf02490cd",
   "metadata": {},
   "source": [
    "# KE-MLM - Log-odd-ratio\n",
    "\n",
    "- use domain dataset as background corpus\n",
    "- use depression as corpus_i\n",
    "- collect top and bottom 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab43e2-c016-44db-8de9-0c834c1734d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_punctuations   = True\n",
    "rm_stopwords      = True\n",
    "save_top_words    = 1500\n",
    "\n",
    "lower_case        = False # already DONE\n",
    "tokenizer         = None # use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5de71f-ba17-4241-8c11-03837c1d6afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# LogOddsRatio Class\n",
    "# \n",
    "# A class for computing Log-odds-ratio with informative Dirichlet priors\n",
    "#\n",
    "# See http://languagelog.ldc.upenn.edu/myl/Monroe.pdf for more detail\n",
    "# \n",
    "#####################################################################\n",
    "\n",
    "__author__ = \"Kornraphop Kawintiranon\"\n",
    "__email__ = \"kornraphop.k@gmail.com\"\n",
    "\n",
    "import math\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import json, pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizerFast, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "# from src.models  import *\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e319f19e-dc0a-4d46-9899-c87318839213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "\n",
    "def parallel_tokenize(corpus, tokenizer=None, n_jobs=-1):\n",
    "    if tokenizer == None:\n",
    "        tokenizer = NLTKWordTokenizer()\n",
    "    if n_jobs < 0:\n",
    "        n_jobs = multiprocessing.cpu_count() - 1\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        corpus_tokenized = list(\n",
    "            tqdm.tqdm(executor.map(tokenizer.tokenize, corpus, chunksize=200), total=len(corpus), desc='Tokenizing')\n",
    "        )\n",
    "    return corpus_tokenized\n",
    "\n",
    "def remove_stopwords(corpus, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        \n",
    "        # print(words[:100])\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # print(words[:100])\n",
    "        # asdfasfasdf\n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "\n",
    "def remove_punctuations(corpus):\n",
    "    punctuations = string.punctuation\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        # remove single punctuations\n",
    "        words = [w for w in words if not w in punctuations]\n",
    "        words = [re.sub(r\"\"\"[()#[\\]#*+\\-/:;<=>@[\\]^_`{|}~\"\\\\.?!$%&]\"\"\", \"\", w) for w in words]      \n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "    \n",
    "def decontract(corpus):\n",
    "    processed_corpus = []\n",
    "    for phrase in tqdm.tqdm(corpus, desc=\"Decontracting\"):\n",
    "        phrase = re.sub(r\"â€™\", \"\\'\", phrase)\n",
    "\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "        processed_corpus.append(phrase)\n",
    "    return processed_corpus\n",
    "\n",
    "def get_word_counts(corpus):\n",
    "    # Initializing Dictionary\n",
    "    d = {}\n",
    "\n",
    "    # Counting number of times each word comes up in list of words (in dictionary)\n",
    "    for words in tqdm.tqdm(corpus, desc=\"Word Counting\"):\n",
    "        for w in words:\n",
    "            d[w] = d.get(w, 0) + 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10024851-a07b-41d3-861a-fe8cbcb6e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogOddsRatio:\n",
    "    \"\"\"\n",
    "    Log-odds-ratio with informative Dirichlet priors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_i, corpus_j, background_corpus=None, lower_case=True, rm_stopwords=True, rm_punctuations=True, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Create a class object and prepare word counts for log-odds-ratio computation\n",
    "        Args:\n",
    "            corpus_i:        A list of documents, each contains a string\n",
    "            corpus_j:        A list of documents, each contains a string\n",
    "            background_corpus (default = None): If None, it will be assigned to a concatenation of `corpus_i` and `corpus_j`\n",
    "            rm_stopwords:    Whether remove stopwords in preprocessing step\n",
    "            tokenizer:       To specify a specific tokenizer for tokenization step\n",
    "        \"\"\"\n",
    "\n",
    "        def preprocessing(corpus):\n",
    "            if lower_case:\n",
    "                print(\"lowercasing\")\n",
    "                corpus = [text.lower() for text in corpus]\n",
    "            corpus = decontract(corpus)\n",
    "            tokenized_corpus = parallel_tokenize(corpus, tokenizer)\n",
    "\n",
    "            if rm_stopwords:\n",
    "                print(\"removing stopwords\")\n",
    "                tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "\n",
    "            if rm_punctuations:\n",
    "                print(\"removing punctuation\")\n",
    "                tokenized_corpus = remove_punctuations(tokenized_corpus)\n",
    "\n",
    "            print(tokenized_corpus[0][:500])\n",
    "\n",
    "            return tokenized_corpus\n",
    "\n",
    "        # Convert a list of string into a list of lists of words\n",
    "        logger.info(\"Preprocessing corpus-i\")\n",
    "        corpus_i = preprocessing(corpus_i)\n",
    "        logger.info(\"Preprocessing corpus-j\")\n",
    "        corpus_j = preprocessing(corpus_j)\n",
    "        if background_corpus != None:\n",
    "            logger.info(\"Preprocessing corpus-background\")\n",
    "            background_corpus = preprocessing(background_corpus)\n",
    "        \n",
    "        # Compute word counts of every words on each corpus separately\n",
    "        logger.info(\"Getting word counts from corpus-i\")\n",
    "        self.y_i = get_word_counts(corpus_i)\n",
    "        logger.info(\"Getting word counts from corpus-j\")\n",
    "        self.y_j = get_word_counts(corpus_j)\n",
    "        logger.info(\"Getting word counts from corpus-background\")\n",
    "        if background_corpus:\n",
    "            self.alpha = get_word_counts(background_corpus)\n",
    "        else:\n",
    "            # Combine words and sum their counts of corpus i and j in case no specified background corpus\n",
    "            self.alpha = {k: self.y_i.get(k, 0) + self.y_j.get(k, 0) for k in set(self.y_i) | set(self.y_j)}\n",
    "\n",
    "        # Sort dicts\n",
    "        logger.debug(\"Start sorting and backing up to files\")\n",
    "        self.y_i = {k: v for k, v in sorted(self.y_i.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.y_j = {k: v for k, v in sorted(self.y_j.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.alpha = {k: v for k, v in sorted(self.alpha.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        # Write to files as backup\n",
    "        with open(\"vocabs_i.txt\", \"w\") as f:\n",
    "            for k, v in self.y_i.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"vocabs_j.txt\", \"w\") as f:\n",
    "            for k, v in self.y_j.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"vocabs_alpha.txt\", \"w\") as f:\n",
    "            for k, v in self.alpha.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "\n",
    "        # Initialize necessary variables\n",
    "        self.delta = None\n",
    "        self.sigma_2 = None\n",
    "        self.z_scores = None\n",
    "\n",
    "        # Compute\n",
    "        logger.info(\"Start computing delta\")\n",
    "        self._compute_delta()\n",
    "        logger.info(\"Start computing sigma^2\")\n",
    "        self._compute_sigma_2()\n",
    "        logger.info(\"Start computing Z-score\")\n",
    "        self._compute_z_scores()\n",
    "\n",
    "        # Sort dicts\n",
    "        logger.debug(\"Start sorting and backing up to files\")\n",
    "        self.delta = {k: v for k, v in sorted(self.delta.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.sigma_2 = {k: v for k, v in sorted(self.sigma_2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        self.z_scores = {k: v for k, v in sorted(self.z_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        # Write to files as backup\n",
    "        with open(\"delta.txt\", \"w\") as f:\n",
    "            for k, v in self.delta.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"sigma_2.txt\", \"w\") as f:\n",
    "            for k, v in self.sigma_2.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "        with open(\"z_scores.txt\", \"w\") as f:\n",
    "            for k, v in self.z_scores.items():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "\n",
    "\n",
    "    def _compute_delta(self):\n",
    "            \"\"\" The usage difference for word w among two corpora i and j\n",
    "            \"\"\"\n",
    "            self.delta = dict()\n",
    "            n_i = sum(self.y_i.values())\n",
    "            n_j = sum(self.y_j.values())\n",
    "            alpha_zero = sum(self.alpha.values())\n",
    "            logger.debug(f\"Size of corpus-i: {n_i}\")\n",
    "            logger.debug(f\"Size of corpus-j: {n_j}\")\n",
    "            logger.debug(f\"Size of background corpus: {alpha_zero}\")\n",
    "\n",
    "            try:\n",
    "                for w in set(self.y_i) | set(self.y_j): # iterate through all words among two corpora\n",
    "\n",
    "                    # print(self.y_i.get(w, 0))\n",
    "                    # print(self.alpha.get(w, 0))\n",
    "                    # print(n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0))\n",
    "\n",
    "                    first_top    = self.y_i.get(w, 0) + self.alpha.get(w, 0)\n",
    "                    first_bottom = n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0)\n",
    "\n",
    "                    second_top    = self.y_j.get(w, 0) + self.alpha.get(w, 0)\n",
    "                    second_bottom = n_j + alpha_zero - self.y_j.get(w, 0) - self.alpha.get(w, 0)\n",
    "\n",
    "\n",
    "                    if first_bottom == 0 and second_bottom == 0:\n",
    "                        first_log  = 0\n",
    "                        second_log = 0\n",
    "\n",
    "                    if first_bottom == 0 and second_bottom != 0:\n",
    "                        first_log  = 0\n",
    "                        second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                    if second_bottom == 0:\n",
    "                        first_log = math.log10( first_top / first_bottom )\n",
    "                        second_log = 0\n",
    "\n",
    "\n",
    "                    if first_bottom != 0 and second_bottom != 0:\n",
    "                        if (first_top / first_bottom) == 0 and (second_top / second_bottom) != 0:\n",
    "                            first_log = 0\n",
    "                            second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                        if (first_top / first_bottom) != 0 and (second_top / second_bottom) == 0:\n",
    "                            first_log  = math.log10( first_top  / first_bottom )\n",
    "                            second_log = 0\n",
    "\n",
    "                        if (first_top / first_bottom) != 0 and (second_top / second_bottom) != 0:\n",
    "                            first_log  = math.log10( first_top  / first_bottom )\n",
    "                            second_log = math.log10( second_top / second_bottom )\n",
    "\n",
    "                    self.delta[w] = first_log - second_log\n",
    "\n",
    "            except ValueError as e:\n",
    "                logger.debug(f\"Y-i of the word {w}:\", self.y_i.get(w, 0))\n",
    "                logger.debug(f\"alpha of the word {w}:\", self.alpha.get(w, 0))\n",
    "                logger.debug(f\"value:\", (self.y_i.get(w, 0) + self.alpha.get(w, 0)) /\n",
    "                      (n_i + alpha_zero - self.y_i.get(w, 0) - self.alpha.get(w, 0)))\n",
    "                raise e\n",
    "\n",
    "    def _compute_sigma_2(self):\n",
    "        \"\"\" Compute estimated values of sigma squared\n",
    "        \"\"\"\n",
    "        self.sigma_2 = dict()\n",
    "        for w in self.delta:\n",
    "            if (self.y_i.get(w, 0) + self.alpha.get(w, 0)) == 0 or (self.y_j.get(w, 0) + self.alpha.get(w, 0)) == 0:\n",
    "                self.sigma_2[w] = 0\n",
    "            else:\n",
    "                self.sigma_2[w] = (1 / (self.y_i.get(w, 0) + self.alpha.get(w, 0))) + (1 / (self.y_j.get(w, 0) + self.alpha.get(w, 0)))\n",
    "\n",
    "    def _compute_z_scores(self):\n",
    "        self.z_scores = dict()\n",
    "        for w in self.delta:\n",
    "            if self.sigma_2.get(w, 0) == 0:\n",
    "                # score 0 is in the middle so it will not show up in top or bottom which is what we want!\n",
    "                self.z_scores[w] = 0\n",
    "            else:\n",
    "                self.z_scores[w] = self.delta.get(w, 0) / math.sqrt(self.sigma_2.get(w, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29563ec0-ac5a-42c2-89d5-4b6cb75bbe4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WITH BG CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aaf9997-b2b4-42ac-92e9-310c187d337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_corpus = pickle.load(open(\"../data/domain/domain_corpus_traindepcon_ratio10.pkl\", \"rb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82b46ea-45ab-49cf-a00b-942edd2fd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ratio = 1\n",
    "\n",
    "all_train_depression_text = pickle.load(open(f\"../data/classi/classi_corpus_traindep_ratio{classi_ratio}.pkl\", \"rb\"))\n",
    "all_train_control_text    = pickle.load(open(f\"../data/classi/classi_corpus_traincon_ratio{classi_ratio}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8f66f0-c22e-4000-93b4-f1df2f642062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:45:27.893 | INFO     | __main__:__init__:37 - Preprocessing corpus-i\n",
      "Decontracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 1034.62it/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:04<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stopwords\n",
      "removing punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:45:33.829 | INFO     | __main__:__init__:39 - Preprocessing corpus-j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boosted', 'self', 'esteem', 'well', 'last', 'bullied', 'lot', 'even', 'girls', 'times', 'one', 'put', 'wtf', 'metamorphis', 'guys', 'put', 'somewhere', 'middle', 'still', 'sometimes', 'decent', 'comebacks', 'even', 'funny', 'times', 'verbal', \"back'n'forts\", 'always', 'gamble', 'comeback', 'sometimes', 'turned', 'physical', 'instead', 'live', 'norway', 'think', 'people', 'misunderstand', 'mean', 'cold', 'mean', 'people', 'prefers', 'maximum', 'celsius', 'inside', 'puts', 'thermostat', 'ridiculously', 'low', 'slightly', 'normal', 'might', 'also', 'misunderstood', 'peeoples', 'preference', 'mean', 'well', 'due', 'this', 'go', 'snowboarding', 'winter', 'sometimes', 'like', 'deep', 'ragehatred', 'little', 'bit', 'chilled', 'air', 'usually', 'stay', 'inside', 'much', 'winter', 'first', 'start', 'freezing', 'takes', 'time', 'get', 'warm', 'again', 'use', 'clothes', 'others', 'due', 'like', 'complain', 'everybody', 'else', 'fine', 'tge', 'temperature', 'person', 'office', 'always', 'changing', 'thermostat', 'sitting', 'sweater', 'annoys', 'friend', 'preferring', 'celsius', 'home', 'usually', 'makes', 'ill', 'days', 'literally', 'end', 'freezing', 'long', 'periods', 'time', 'even', 'sweatersblankets', 'stuff', 'would', 'nice', 'find', 'middle', 'ground', 'stick', 'wool', 'sweater', 'fine', 'sits', 'tshirt', 'guess', 'believe', 'fine', 'know', 'prefer', 'couple', 'degrees', 'iver', 'normal', 'room', 'temperature', 'dude', 'brother', 'rent', 'together', 'keep', 'windows', 'open', 'seemingly', 'times', 'even', 'winter', 'never', 'cold', 'office', 'one', 'people', 'likes', 'cold', 'obviously', 'way', 'cold', 'keeps', 'sweater', 'anything', 'slightly', 'normal', 'temp', 'nkt', 'one', 'complained', 'least', 'liveable', 'comparison', 'know', 'temperature', \"y'all\", 'mind', 'cold', 'chilled', 'reasonable', 'mess', 'thermostat', 'though', 'maybe', 'picky', 'sure', 'hell', 'sucks', 'freeze', 'time', 'jesus', 'fuck', 'stupid', 'sad', 'guy', 'loves', 'travel', 'amazing', 'see', 'shit', 'app', 'times', 'go', 'unnoticed', 'usually', 'first', 'thing', 'people', 'advertise', 'for', 'come', 'across', 'really', 'cool', 'art', 'due', 'it', 'gets', 'removed', 'greater', 'loss', 'anything', 'else', 'especially', 'artists', 'wants', 'work', 'seen', 'many', 'people', 'possible', 'understand', 'cemeteries', 'left', 'alone', 'people', 'grieving', 'lost', 'ones', 'able', 'certain', 'amount', 'peace', 'there', 'dope', 'shit', 'heard', 'never', 'got', 'about', 'definitely', 'downloading', 'get', 'home', 'thanks', 'suggestion', 'nice', 'hear', 'went', 'well', 'managed', 'hide', 'part', 'cake', 'guidance', 'counselor', 'amazing', 'well', 'struggled', 'really', 'fucking', 'bad', 'last', 'years', 'high', 'school', 'failing', 'pretty', 'bad', 'really', 'bad', 'case', 'extremely', 'tired', 'school', 'point', 'could', 'sit', 'homework', 'assignments', 'hours', 'straight', 'able', 'focus', 'anything', 'home', 'even', 'though', 'books', 'front', 'me', 'find', 'interest', 'history', 'gym', 'languages', 'couple', 'subjects', 'well', 'math', 'couple', 'others', 'disaster', 'guidance', 'counselor', 'figured', 'classes', 'focus', 'helped', 'get', 'leave', 'classes', 'important', 'example', 'went', 'waldorf', 'eurythmy', 'chorus', 'school', 'everyone', 'attend', 'two', 'hours', 'week', 'got', 'leave', 'could', 'use', 'catch', 'important', 'ones', 'many', 'days', 'would', 'away', 'school', 'would', 'get', 'limit', 'able', 'graduate', 'even', 'decent', 'test', 'scores', 'on', 'guidance', 'counselors', 'help', 'grace', 'english', 'language', 'teacher', 'managed', 'graduate', 'took', 'advice', 'gave', 'working', 'couple', 'years', 'pursuing', 'higher', 'education', 'yet', 'pick', 'higher', 'education', 'since', 'things', 'worked', 'even', 'remarkable', 'low', 'grades', 'least', 'chance', 'go', 'high', 'school', 'take', 'private', 'exams', 'beside', 'ones', 'might', 'want', 'get', 'complete', 'paper', 'guidance', 'counselor', 'also', 'gave', 'information', 'schools', 'public', 'private', 'could', 'attend', 'night', 'classes', 'certain', 'subjects', 'ones', 'great', 'city', 'along', 'town', 'case', 'graduation', 'person', 'helped', 'shitload', 'people', 'difficult', 'situations', 'getting', 'degree', 'along', 'helping', 'great', 'students', 'amazing', 'results', 'tests', 'schools', 'universities', 'expect', 'get', 'along', 'careers', 'realistically', 'pursue', 'job', 'market', 'likely', 'look', 'like', 'done', 'guidance', 'counselor', 'never', 'told', 'anyone', 'chance', 'fear', 'destroying', 'motivation', 'likely', 'get', 'scores', 'never', 'hurts', 'try', 'long', 'school', 'applying', 'for', 'person', 'thoroughly', 'enjoyed', 'work', 'would', 'sometimes', 'help', 'students', 'networking', 'well', 'really', 'fuck', 'wanted', 'believe', 'ye', 'bro', 'giving', 'zero', 'fucks', 'danger', 'everything', 'bro', 'needs', 'help', 'heard', 'stuff', 'ella']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decontracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 2984.31it/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:01<00:00, 84.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stopwords\n",
      "removing punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:45:36.075 | INFO     | __main__:__init__:42 - Preprocessing corpus-background\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['regular', 'tv', 'first', 'session', 'went', 'great', 'new', 'player', 'already', 'hooked', 'taaru', 'village', 'wisdom', 'halfling', 'druid', 'blend', 'outlande', 'hume', 'imperial', 'spy', 'human', 'rogue', 'criminalspy', 'variant', 'elvaanblooded', 'hunter', 'halfelf', 'ranger', 'outlander', 'woke', 'shipwrecked', 'beach', 'knowledge', 'got', 'there', 'combing', 'wreckage', 'washing', 'beach', 'would', 'sometimes', 'find', 'trinkets', 'items', 'triggered', 'memories', 'played', 'players', 'flashbacks', 'flashbacks', 'used', 'help', 'connect', 'characters', 'other', 'storm', 'rolled', 'horizon', 'water', 'near', 'beach', 'began', 'boil', 'many', 'dozens', 'fish', 'floated', 'belly', 'up', 'massive', 'flametouched', 'giant', 'crab', 'surfaced', 'bum', 'rushed', 'druid', 'searching', 'along', 'beach', 'rogue', 'ranger', 'putting', 'together', 'simple', 'shelter', 'using', 'sailcloth', 'recovered', 'close', 'fight', 'thick', 'shell', 'megalocrab', 'staving', 'many', 'deadly', 'attacks', 'party', 'celebrated', 'victory', 'delicious', 'feast', 'boiled', 'crab', 'meat', 'recovering', 'exertion', 'combat', 'tending', 'wounds', 'hunkered', 'shelter', 'storm', 'driven', 'rain', 'wind', 'made', 'near', 'impossible', 'see', 'despite', 'conditions', 'jungle', 'goblins', 'drawn', 'scent', 'cooked', 'crab', 'found', 'party', 'attempted', 'take', 'captive', 'driven', 'half', 'numbers', 'laid', 'low', 'weary', 'adventurers', 'tv', 'gametable', 'huge', 'hit', 'new', 'player', 'returning', 'player', 'played', 'ten', 'years', 'real', 'problem', 'route', 'inexplicably', 'died', 'took', 'chromecast', 'it', 'fortunately', 'quick', 'reset', 'fixed', 'problem', 'still', 'working', 'character', 'creation', 'middle', 'battle', 'also', 'discovered', 'need', 'use', 'kind', 'tokens', 'moving', 'npc', 'around', 'map', 'easier', 'move', 'roll20', 'first', 'hidden', 'movement', 'gm', 'layer', 'instead', 'trying', 'use', 'miniatures', 'map', 'trying', 'use', 'arrow', 'pointer', 'roll20', 'point', 'things', 'players', 'lack', 'monster', 'tokens', 'made', 'difficult', 'times', 'wait', 'next', 'game', 'place', 'become', 'dank', 'musty', 'fungal', 'infested', 'ruin', 'lots', 'myconids', 'spore', 'bats', 'bog', 'giants', 'think', 'zangarmarsh', 'world', 'warcraft', 'areas', 'oozes', 'puddings', 'lichens', 'water', 'guppies', 'love', 'water', 'acrylic', 'sheet', 'protect', 'general', 'bumps', 'scrapes', 'going', 'put', 'huge', 'heavy', 'terrain', 'props', 'top', 'sorta', 'campaign', 'creation', 'players', 'woke', 'beach', 'memory', 'got', 'there', 'first', 'session', 'mostly', 'flashbacks', 'played', 'characters', 'providing', 'connections', 'characters', 'think', 'okay', 'now', 'probably', 'problem', 'people', 'get', 'used', 'using', 'battlemap', 'get', 'comfortable', 'around', 'it', 'cause', 'need', 'yet', 'another', 'jail', 'spice', 'water', 'must', 'flow', 'adding', 'proficiency', 'bonus', 'right', 'rolling', '6', 'hit', 'spell', 'dc', 'phandelver', 'part', 'sword', 'coast', 'faerun', 'forgotten', 'realms', 'campaign', 'setting', 'far', 'information', 'available', 'campaign', 'setting', 'ever', 'able', 'get', 'lifetime', 'gaming', 'huge', 'dungeon', 'maps', 'went', 'digital', 'map', 'solution', 'even', 'face', 'face', 'gaming', 'long', 'term', 'costs', 'e', 'works', 'fine', 'lot', 'complaints', 'stem', 'nontraditional', 'power', 'system', 'works', 'great', 'length', 'combat', 'encounters', 'majority', 'problems', 'length', 'combat', 'dealt', 'getting', 'players', 'actually', 'prepare', 'turns', 'ahead', 'time', 'understand', 'powers', 'actually', 'work', 'also', 'mitigate', 'length', 'combat', 'encounters', 'halving', 'monster', 'hit', 'points', 'giving', 'bit', 'damage', 'bonus', 'based', 'level', '1', '3', 'heroic', 'tier', 'makes', 'combats', 'little', 'dangerous', 'faster', 'mostly', 'applies', 'monsters', 'first', 'two', 'monster', 'manuals', 'mm3', 'created', 'using', 'new', 'design', 'methodology', 'monsters', 'dangerous', 'fewer', 'hp', 'believe', 'finally', 'give', 'everyone', 'choice', 'expertise', 'feat', 'free', 'fix', 'math', 'expects', 'characters', 'take', 'feat', 'increases', 'attack', 'bonus', '1', 'per', 'tier', 'otherwise', 'feat', 'tax', 'math', 'behind', 'monster', 'ac', 'basically', 'made', 'assumption', 'players', 'would', 'taking', 'expertise', 'feats', 'hence', 'feats', 'feat', 'tax', 'spying', 'former', 'captain', 'dragging', 'druid', 'younger', 'sister', 'away', 'chains', 'jungle', 'party', 'follows', 'morning', 'choosing', 'rest', 'instead', 'charging', 'blindly', 'jungle', 'night', 'morning', 'quickly', 'beset', 'upon', 'giant', 'carnivorous', 'mantrap', 'plants', 'almost', 'losing', 'rogue', 'one', 'trying', 'burrow', 'away', 'prey', 'shorter', 'encounter', 'goblins', 'later', 'giant', 'constrictor', 'snake', 'whittled', 'party', 'resources', 'leaving', 'wary', 'passed', 'writhing', 'gulch', 'horde', 'assassin', 'vines', 'almost', 'overwhelmed', 'adventurer', 'forcing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decontracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5403/5403 [00:13<00:00, 409.27it/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5403/5403 [02:17<00:00, 39.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stopwords\n",
      "removing punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:49:33.599 | INFO     | __main__:__init__:46 - Getting word counts from corpus-i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought', 'dlc', 'fighting', 'first', 'time', 'tried', 'several', 'times', 'seem', 'get', 'him', 'edit', 'us', 'mountain', 'timezone', 'foreseeable', 'future', 'edit', 'beat', 'him', 'needed', 'green', 'blossoms', 'blocking', 'dodging', 'turns', 'out', 'playing', 'couple', 'years', 'want', 'find', 'song', 'challenges', 'enables', 'learn', 'also', 'fun', 'play', 'best', 'pianist', 'long', 'shot', 'trying', 'better', 'myself', 'probably', 'difficult', 'song', 'learned', 'far', 'doctor', 'murray', 'gold', 'link', 'video', 'mark', 'fowler', 'playing', 'give', 'idea', 'sounds', 'like', 'never', 'able', 'play', 'quite', 'well', 'think', 'alright', 'suggestions', 'songs', 'little', 'difficulty', 'would', 'great', 'sorry', 'posted', 'wrong', 'place', 'damn', 'sounds', 'good', 'thought', 'would', 'watched', 'videos', 'thanks', 'comment', 'definitely', 'try', 'lol', 'wut', 'correct', 'thinking', 'excommunication', 'remove', 'name', 'records', 'apparently', 'neither', 'resigning', 'exactly', 'advertised', 'basically', 'way', 'remove', 'name', 'records', 'dead', 'session', 'got', 'kicked', 'from', 'kicked', 'made', 'private', 'session', 'this', 'unfortunately', 'invites', 'working', 'also', 'missed', 'out', 'comment', 'written', 'hours', 'ago', 'collection', 'every', 'time', 'drinkin', 'said', 'last', 'night', 'enjoy', 'drinkin', 'drankin', 'somebody', 'totally', 'remix', 'this', 'edit', 'changed', 'freely', 'downloadable', 'anyone', 'willing', 'give', 'shot', 'go', 'ahead', 'living', 'dark', 'ages', 'use', 'posting', 'gif', 'long', 'use', 'webm', 'gfycat', 'swear', 'read', 'reddit', 'like', 'months', 'ago', 'almost', 'identical', 'words', 'greentext', 'sugercane', 'cactus', 'max', 'height', 'unless', 'manually', 'place', 'more', 'til', 'manly', 'accents', 'imagine', 'people', 'itt', 'pay', 'compliment', 'immediately', 'pull', 'mobile', 'begin', 'typing', 'confirmed', 'feminist', 'chaos', 'reinforced', 'club', 'works', 'great', 'keep', 'humanity', 'would', 'definitely', 'stay', 'away', 'normal', '15', 'raw', 'weapon', 'paths', 'elemental', 'paths', 'good', 'edit', 'armor', 'really', 'matter', 'would', 'try', 'stay', 'fast', 'rolling', 'able', 'take', 'many', 'hits', 'even', 'full', 'havels', 'try', 'going', 'mugen', 'monkey', 'sort', 'armor', 'poise', 'find', 'max', 'amount', 'sl1', 'get', 'elsanna', 'kristanna', 'jelsa', 'recall', 'character', 'name', 'starting', 'j', 'ah', 'thanks', 'catarina', 'kittycat', 'daniel', 'tiger', 'neighborhood', 'like', 'wtf', 'bit', 'leveled', 'unless', 'staying', 'low', 'leveled', 'make', 'challenging', 'suggest', 'leveling', 'times', 'always', 'wait', 'defeat', 'kings', 'killed', 'seathe', 'bed', 'chaos', 'nito', 'killed', 'yet', 'would', 'give', 'easy', 'levels', 'actually', 'help', 'gaming', 'pc', 'commission', 'right', 'would', 'running', 'fps', 'good', 'luck', 'call', 'maryjane', 'gamertag', 'cripplecow', 'case', 'hundreds', 'people', 'winning', 'lottery', 'everyone', 'got', 'fortune', 'cookie', 'actually', 'correct', 'number', 'even', 'play', 'lottery', 'number', 'fortune', 'cookie', 'wins', 'split', 'money', 'several', 'hundred', 'people', 'really', 'worth', 'it', 'think', 'one', 'times', 'fortune', 'cookie', 'ever', 'right', 'number', 'well', 'idiot', 'got', 'stepped', 'cow', 'crushed', 'knee', 'name', 'kind', 'joke', 'seeing', 'crippled', 'cow', 'gamertag', 'cripplecow', 'need', 'help', 'free', 'sure', 'sounds', 'good', 'me', 'get', 'daggerfall', 'free', 'bethesda', 'website', 'one', 'best', 'tes', 'games', 'highly', 'recommend', 'downloading', 'it', 'speaking', 'experience', 'got', 'game', 'last', 'update', 'almost', 'positive', 'keep', 'save', 'files', 'never', 'seen', 'game', 'not', 'understand', 'second', 'part', 'question', 'though', 'mind', 'elaborating', 'still', 'trouble', 'level', 'could', 'help', 'out', 'okay', 'free', 'basically', 'day', 'problem', 'sweet', 'many', 'ad', 'references', 'thread', 'man', 'actually', 'really', 'enjoyed', 'love', 'monsters', 'last', 'minutes', 'alien', 'exposed', 'still', 'think', 'bad', 'episode', 'okay', 'happen', 'think', 'might', 'giantdad', 'yup', 'alright', 'then', 'similar', 'thing', 'happened', 'though', 'believe', 'test', 'post', 'please', 'ignore', 'beaten', 'edit', 'apparently', 'controversial', 'comment', 'time', 'story', 'time', 'played', 'game', 'jump', 'campfires', 'one', 'poor', 'guy', 'tripped', 'fell', 'in', 'know', 'soon', 'saw', 'little', 'burst', 'laughing', 'picking', 'putting', 'down', 'payed', 'galathil', 'septims', 'change', 'appearance', 'th', 'time', 'something', 'unexpected', 'happened', 'think', 'moving', 'accepted', 'racemenu', 'opened', 'kept', 'moving', 'place', 'editing', 'character', 'try', 'report', 'back', 'gif', 'edit', 'took', 'find', 'required', 'items', 'record', 'use', 'photoshop', 'turn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word Counting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 893.76it/s] \n",
      "2022-06-03 15:49:33.775 | INFO     | __main__:__init__:48 - Getting word counts from corpus-j\n",
      "Word Counting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 2175.85it/s]\n",
      "2022-06-03 15:49:33.849 | INFO     | __main__:__init__:50 - Getting word counts from corpus-background\n",
      "Word Counting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5403/5403 [00:18<00:00, 293.95it/s]\n",
      "2022-06-03 15:49:52.233 | DEBUG    | __main__:__init__:58 - Start sorting and backing up to files\n",
      "2022-06-03 15:49:55.616 | INFO     | __main__:__init__:80 - Start computing delta\n",
      "2022-06-03 15:49:55.628 | DEBUG    | __main__:_compute_delta:112 - Size of corpus-i: 715822\n",
      "2022-06-03 15:49:55.630 | DEBUG    | __main__:_compute_delta:113 - Size of corpus-j: 262324\n",
      "2022-06-03 15:49:55.630 | DEBUG    | __main__:_compute_delta:114 - Size of background corpus: 67882756\n",
      "2022-06-03 15:49:55.805 | INFO     | __main__:__init__:82 - Start computing sigma^2\n",
      "2022-06-03 15:49:55.904 | INFO     | __main__:__init__:84 - Start computing Z-score\n",
      "2022-06-03 15:49:55.952 | DEBUG    | __main__:__init__:88 - Start sorting and backing up to files\n",
      "2022-06-03 15:49:57.741 | INFO     | __main__:<module>:19 - Saving top and bottom 1500 words ranked by Z-score\n"
     ]
    }
   ],
   "source": [
    "# DEPRESSION = i\n",
    "corpus_i  = all_train_depression_text\n",
    "corpus_j  = all_train_control_text\n",
    "background_corpus = background_corpus\n",
    "\n",
    "log_odds_ratio = LogOddsRatio(corpus_i          = corpus_i,\n",
    "                              corpus_j          = corpus_j, \n",
    "                              background_corpus = background_corpus,\n",
    "                              lower_case        = lower_case, \n",
    "                              rm_stopwords      = rm_stopwords, \n",
    "                              rm_punctuations   = rm_punctuations, \n",
    "                              tokenizer         = None)\n",
    "\n",
    "# Save top words into a file\n",
    "if save_top_words != None and save_top_words > 0:\n",
    "    if save_top_words > len(log_odds_ratio.z_scores):\n",
    "        raise ValueError(\"--save_top_words must be less than or equal to vocab size\")\n",
    "\n",
    "    logger.info(f\"Saving top and bottom {save_top_words} words ranked by Z-score\")\n",
    "    tops    = list(log_odds_ratio.z_scores.keys())[:save_top_words]\n",
    "    bottoms = list(log_odds_ratio.z_scores.keys())[-save_top_words:]\n",
    "\n",
    "    with open(f\"./01-logodds-topbot{save_top_words}-R{classi_ratio}-nostops.txt\", \"w\") as f:\n",
    "        for word in tops:\n",
    "            f.write(word + \"\\n\")\n",
    "        for word in bottoms:\n",
    "            f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
