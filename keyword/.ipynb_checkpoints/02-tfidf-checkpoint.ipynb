{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASKER KEYWORD - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# LogOddsRatio Class\n",
    "# \n",
    "# A class for computing Log-odds-ratio with informative Dirichlet priors\n",
    "#\n",
    "# See http://languagelog.ldc.upenn.edu/myl/Monroe.pdf for more detail\n",
    "# \n",
    "#####################################################################\n",
    "\n",
    "__author__ = \"Kornraphop Kawintiranon\"\n",
    "__email__ = \"kornraphop.k@gmail.com\"\n",
    "\n",
    "import math\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import json, pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizerFast, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "# from src.models  import *\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_stopwords      = True\n",
    "rm_punctuations   = True\n",
    "save_top_words    = 3000\n",
    "\n",
    "lower_case        = False # already DONE\n",
    "tokenizer         = None # use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_ratio = 1\n",
    "\n",
    "depression_text = pickle.load(open(f\"../data/classi/classi_corpus_traindep_ratio{classi_ratio}.pkl\", \"rb\"))\n",
    "control_text    = pickle.load(open(f\"../data/classi/classi_corpus_traincon_ratio{classi_ratio}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "\n",
    "def parallel_tokenize(corpus, tokenizer=None, n_jobs=-1):\n",
    "    if tokenizer == None:\n",
    "        tokenizer = NLTKWordTokenizer()\n",
    "    if n_jobs < 0:\n",
    "        n_jobs = multiprocessing.cpu_count() - 1\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        corpus_tokenized = list(\n",
    "            tqdm.tqdm(executor.map(tokenizer.tokenize, corpus, chunksize=200), total=len(corpus), desc='Tokenizing')\n",
    "        )\n",
    "    return corpus_tokenized\n",
    "\n",
    "def remove_stopwords(corpus, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        \n",
    "        # print(words[:100])\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # print(words[:100])\n",
    "        # asdfasfasdf\n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "\n",
    "def remove_punctuations(corpus):\n",
    "    punctuations = string.punctuation\n",
    "    processed_corpus = []\n",
    "    for words in corpus:\n",
    "        # remove single punctuations\n",
    "        words = [w for w in words if not w in punctuations]\n",
    "        words = [re.sub(r\"\"\"[()#[\\]#*+\\-/:;<=>@[\\]^_`{|}~\"\\\\.?!$%&]\"\"\", \"\", w) for w in words]      \n",
    "        processed_corpus.append(words)\n",
    "    return processed_corpus\n",
    "    \n",
    "def decontract(corpus):\n",
    "    processed_corpus = []\n",
    "    for phrase in tqdm.tqdm(corpus, desc=\"Decontracting\"):\n",
    "        phrase = re.sub(r\"’\", \"\\'\", phrase)\n",
    "\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "        processed_corpus.append(phrase)\n",
    "    return processed_corpus\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    if lower_case:\n",
    "        print(\"lowercasing\")\n",
    "        corpus = [text.lower() for text in corpus]\n",
    "    corpus = decontract(corpus)\n",
    "    tokenized_corpus = parallel_tokenize(corpus, tokenizer)\n",
    "\n",
    "    if rm_stopwords:\n",
    "        print(\"removing stopwords\")\n",
    "        tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "        \n",
    "    if rm_punctuations:\n",
    "        print(\"removing punctuation\")\n",
    "        tokenized_corpus = remove_punctuations(tokenized_corpus)\n",
    "        \n",
    "    print(tokenized_corpus[0][:500])\n",
    "\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decontracting: 100%|██████████| 153/153 [00:00<00:00, 1059.29it/s]\n",
      "Tokenizing: 100%|██████████| 153/153 [00:04<00:00, 31.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stopwords\n",
      "removing punctuation\n",
      "['boosted', 'self', 'esteem', 'well', 'last', 'bullied', 'lot', 'even', 'girls', 'times', 'one', 'put', 'wtf', 'metamorphis', 'guys', 'put', 'somewhere', 'middle', 'still', 'sometimes', 'decent', 'comebacks', 'even', 'funny', 'times', 'verbal', \"back'n'forts\", 'always', 'gamble', 'comeback', 'sometimes', 'turned', 'physical', 'instead', 'live', 'norway', 'think', 'people', 'misunderstand', 'mean', 'cold', 'mean', 'people', 'prefers', 'maximum', 'celsius', 'inside', 'puts', 'thermostat', 'ridiculously', 'low', 'slightly', 'normal', 'might', 'also', 'misunderstood', 'peeoples', 'preference', 'mean', 'well', 'due', 'this', 'go', 'snowboarding', 'winter', 'sometimes', 'like', 'deep', 'ragehatred', 'little', 'bit', 'chilled', 'air', 'usually', 'stay', 'inside', 'much', 'winter', 'first', 'start', 'freezing', 'takes', 'time', 'get', 'warm', 'again', 'use', 'clothes', 'others', 'due', 'like', 'complain', 'everybody', 'else', 'fine', 'tge', 'temperature', 'person', 'office', 'always', 'changing', 'thermostat', 'sitting', 'sweater', 'annoys', 'friend', 'preferring', 'celsius', 'home', 'usually', 'makes', 'ill', 'days', 'literally', 'end', 'freezing', 'long', 'periods', 'time', 'even', 'sweatersblankets', 'stuff', 'would', 'nice', 'find', 'middle', 'ground', 'stick', 'wool', 'sweater', 'fine', 'sits', 'tshirt', 'guess', 'believe', 'fine', 'know', 'prefer', 'couple', 'degrees', 'iver', 'normal', 'room', 'temperature', 'dude', 'brother', 'rent', 'together', 'keep', 'windows', 'open', 'seemingly', 'times', 'even', 'winter', 'never', 'cold', 'office', 'one', 'people', 'likes', 'cold', 'obviously', 'way', 'cold', 'keeps', 'sweater', 'anything', 'slightly', 'normal', 'temp', 'nkt', 'one', 'complained', 'least', 'liveable', 'comparison', 'know', 'temperature', \"y'all\", 'mind', 'cold', 'chilled', 'reasonable', 'mess', 'thermostat', 'though', 'maybe', 'picky', 'sure', 'hell', 'sucks', 'freeze', 'time', 'jesus', 'fuck', 'stupid', 'sad', 'guy', 'loves', 'travel', 'amazing', 'see', 'shit', 'app', 'times', 'go', 'unnoticed', 'usually', 'first', 'thing', 'people', 'advertise', 'for', 'come', 'across', 'really', 'cool', 'art', 'due', 'it', 'gets', 'removed', 'greater', 'loss', 'anything', 'else', 'especially', 'artists', 'wants', 'work', 'seen', 'many', 'people', 'possible', 'understand', 'cemeteries', 'left', 'alone', 'people', 'grieving', 'lost', 'ones', 'able', 'certain', 'amount', 'peace', 'there', 'dope', 'shit', 'heard', 'never', 'got', 'about', 'definitely', 'downloading', 'get', 'home', 'thanks', 'suggestion', 'nice', 'hear', 'went', 'well', 'managed', 'hide', 'part', 'cake', 'guidance', 'counselor', 'amazing', 'well', 'struggled', 'really', 'fucking', 'bad', 'last', 'years', 'high', 'school', 'failing', 'pretty', 'bad', 'really', 'bad', 'case', 'extremely', 'tired', 'school', 'point', 'could', 'sit', 'homework', 'assignments', 'hours', 'straight', 'able', 'focus', 'anything', 'home', 'even', 'though', 'books', 'front', 'me', 'find', 'interest', 'history', 'gym', 'languages', 'couple', 'subjects', 'well', 'math', 'couple', 'others', 'disaster', 'guidance', 'counselor', 'figured', 'classes', 'focus', 'helped', 'get', 'leave', 'classes', 'important', 'example', 'went', 'waldorf', 'eurythmy', 'chorus', 'school', 'everyone', 'attend', 'two', 'hours', 'week', 'got', 'leave', 'could', 'use', 'catch', 'important', 'ones', 'many', 'days', 'would', 'away', 'school', 'would', 'get', 'limit', 'able', 'graduate', 'even', 'decent', 'test', 'scores', 'on', 'guidance', 'counselors', 'help', 'grace', 'english', 'language', 'teacher', 'managed', 'graduate', 'took', 'advice', 'gave', 'working', 'couple', 'years', 'pursuing', 'higher', 'education', 'yet', 'pick', 'higher', 'education', 'since', 'things', 'worked', 'even', 'remarkable', 'low', 'grades', 'least', 'chance', 'go', 'high', 'school', 'take', 'private', 'exams', 'beside', 'ones', 'might', 'want', 'get', 'complete', 'paper', 'guidance', 'counselor', 'also', 'gave', 'information', 'schools', 'public', 'private', 'could', 'attend', 'night', 'classes', 'certain', 'subjects', 'ones', 'great', 'city', 'along', 'town', 'case', 'graduation', 'person', 'helped', 'shitload', 'people', 'difficult', 'situations', 'getting', 'degree', 'along', 'helping', 'great', 'students', 'amazing', 'results', 'tests', 'schools', 'universities', 'expect', 'get', 'along', 'careers', 'realistically', 'pursue', 'job', 'market', 'likely', 'look', 'like', 'done', 'guidance', 'counselor', 'never', 'told', 'anyone', 'chance', 'fear', 'destroying', 'motivation', 'likely', 'get', 'scores', 'never', 'hurts', 'try', 'long', 'school', 'applying', 'for', 'person', 'thoroughly', 'enjoyed', 'work', 'would', 'sometimes', 'help', 'students', 'networking', 'well', 'really', 'fuck', 'wanted', 'believe', 'ye', 'bro', 'giving', 'zero', 'fucks', 'danger', 'everything', 'bro', 'needs', 'help', 'heard', 'stuff', 'ella']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decontracting: 100%|██████████| 153/153 [00:00<00:00, 2989.90it/s]\n",
      "Tokenizing: 100%|██████████| 153/153 [00:01<00:00, 86.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stopwords\n",
      "removing punctuation\n",
      "['regular', 'tv', 'first', 'session', 'went', 'great', 'new', 'player', 'already', 'hooked', 'taaru', 'village', 'wisdom', 'halfling', 'druid', 'blend', 'outlande', 'hume', 'imperial', 'spy', 'human', 'rogue', 'criminalspy', 'variant', 'elvaanblooded', 'hunter', 'halfelf', 'ranger', 'outlander', 'woke', 'shipwrecked', 'beach', 'knowledge', 'got', 'there', 'combing', 'wreckage', 'washing', 'beach', 'would', 'sometimes', 'find', 'trinkets', 'items', 'triggered', 'memories', 'played', 'players', 'flashbacks', 'flashbacks', 'used', 'help', 'connect', 'characters', 'other', 'storm', 'rolled', 'horizon', 'water', 'near', 'beach', 'began', 'boil', 'many', 'dozens', 'fish', 'floated', 'belly', 'up', 'massive', 'flametouched', 'giant', 'crab', 'surfaced', 'bum', 'rushed', 'druid', 'searching', 'along', 'beach', 'rogue', 'ranger', 'putting', 'together', 'simple', 'shelter', 'using', 'sailcloth', 'recovered', 'close', 'fight', 'thick', 'shell', 'megalocrab', 'staving', 'many', 'deadly', 'attacks', 'party', 'celebrated', 'victory', 'delicious', 'feast', 'boiled', 'crab', 'meat', 'recovering', 'exertion', 'combat', 'tending', 'wounds', 'hunkered', 'shelter', 'storm', 'driven', 'rain', 'wind', 'made', 'near', 'impossible', 'see', 'despite', 'conditions', 'jungle', 'goblins', 'drawn', 'scent', 'cooked', 'crab', 'found', 'party', 'attempted', 'take', 'captive', 'driven', 'half', 'numbers', 'laid', 'low', 'weary', 'adventurers', 'tv', 'gametable', 'huge', 'hit', 'new', 'player', 'returning', 'player', 'played', 'ten', 'years', 'real', 'problem', 'route', 'inexplicably', 'died', 'took', 'chromecast', 'it', 'fortunately', 'quick', 'reset', 'fixed', 'problem', 'still', 'working', 'character', 'creation', 'middle', 'battle', 'also', 'discovered', 'need', 'use', 'kind', 'tokens', 'moving', 'npc', 'around', 'map', 'easier', 'move', 'roll20', 'first', 'hidden', 'movement', 'gm', 'layer', 'instead', 'trying', 'use', 'miniatures', 'map', 'trying', 'use', 'arrow', 'pointer', 'roll20', 'point', 'things', 'players', 'lack', 'monster', 'tokens', 'made', 'difficult', 'times', 'wait', 'next', 'game', 'place', 'become', 'dank', 'musty', 'fungal', 'infested', 'ruin', 'lots', 'myconids', 'spore', 'bats', 'bog', 'giants', 'think', 'zangarmarsh', 'world', 'warcraft', 'areas', 'oozes', 'puddings', 'lichens', 'water', 'guppies', 'love', 'water', 'acrylic', 'sheet', 'protect', 'general', 'bumps', 'scrapes', 'going', 'put', 'huge', 'heavy', 'terrain', 'props', 'top', 'sorta', 'campaign', 'creation', 'players', 'woke', 'beach', 'memory', 'got', 'there', 'first', 'session', 'mostly', 'flashbacks', 'played', 'characters', 'providing', 'connections', 'characters', 'think', 'okay', 'now', 'probably', 'problem', 'people', 'get', 'used', 'using', 'battlemap', 'get', 'comfortable', 'around', 'it', 'cause', 'need', 'yet', 'another', 'jail', 'spice', 'water', 'must', 'flow', 'adding', 'proficiency', 'bonus', 'right', 'rolling', '6', 'hit', 'spell', 'dc', 'phandelver', 'part', 'sword', 'coast', 'faerun', 'forgotten', 'realms', 'campaign', 'setting', 'far', 'information', 'available', 'campaign', 'setting', 'ever', 'able', 'get', 'lifetime', 'gaming', 'huge', 'dungeon', 'maps', 'went', 'digital', 'map', 'solution', 'even', 'face', 'face', 'gaming', 'long', 'term', 'costs', 'e', 'works', 'fine', 'lot', 'complaints', 'stem', 'nontraditional', 'power', 'system', 'works', 'great', 'length', 'combat', 'encounters', 'majority', 'problems', 'length', 'combat', 'dealt', 'getting', 'players', 'actually', 'prepare', 'turns', 'ahead', 'time', 'understand', 'powers', 'actually', 'work', 'also', 'mitigate', 'length', 'combat', 'encounters', 'halving', 'monster', 'hit', 'points', 'giving', 'bit', 'damage', 'bonus', 'based', 'level', '1', '3', 'heroic', 'tier', 'makes', 'combats', 'little', 'dangerous', 'faster', 'mostly', 'applies', 'monsters', 'first', 'two', 'monster', 'manuals', 'mm3', 'created', 'using', 'new', 'design', 'methodology', 'monsters', 'dangerous', 'fewer', 'hp', 'believe', 'finally', 'give', 'everyone', 'choice', 'expertise', 'feat', 'free', 'fix', 'math', 'expects', 'characters', 'take', 'feat', 'increases', 'attack', 'bonus', '1', 'per', 'tier', 'otherwise', 'feat', 'tax', 'math', 'behind', 'monster', 'ac', 'basically', 'made', 'assumption', 'players', 'would', 'taking', 'expertise', 'feats', 'hence', 'feats', 'feat', 'tax', 'spying', 'former', 'captain', 'dragging', 'druid', 'younger', 'sister', 'away', 'chains', 'jungle', 'party', 'follows', 'morning', 'choosing', 'rest', 'instead', 'charging', 'blindly', 'jungle', 'night', 'morning', 'quickly', 'beset', 'upon', 'giant', 'carnivorous', 'mantrap', 'plants', 'almost', 'losing', 'rogue', 'one', 'trying', 'burrow', 'away', 'prey', 'shorter', 'encounter', 'goblins', 'later', 'giant', 'constrictor', 'snake', 'whittled', 'party', 'resources', 'leaving', 'wary', 'passed', 'writhing', 'gulch', 'horde', 'assassin', 'vines', 'almost', 'overwhelmed', 'adventurer', 'forcing']\n"
     ]
    }
   ],
   "source": [
    "depression_text = preprocessing(depression_text)\n",
    "control_text    = preprocessing(control_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print(len(depression_text))\n",
    "print(len(control_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# combine the data into 2 document, one document for depression and one document for control group\n",
    "def get_tfidf_keyword(depression_text, control_text):\n",
    "\n",
    "    class_docs = [''] * 2  # concat all texts for each class\n",
    "    \n",
    "    # control = class 0\n",
    "    for text in control_text:\n",
    "        for word in text:\n",
    "            word = word + ' '\n",
    "            class_docs[0] += word\n",
    "    print(len(class_docs[0]))\n",
    "    \n",
    "    # depression = class 1\n",
    "    for text in depression_text:\n",
    "        for word in text:\n",
    "            word = word + ' '\n",
    "            class_docs[1] += word\n",
    "    print(len(class_docs[1]))\n",
    "\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    feat  = tfidf.fit_transform(class_docs).todense()  # (n_classes, vocabs)\n",
    "    feat  = np.squeeze(np.asarray(feat))  # matrix -> array\n",
    "            \n",
    "    # ---------- Control ----------\n",
    "    con_sorted_idx = feat[0].argsort()[::-1]\n",
    "    control_keyword = [tfidf.get_feature_names()[idx] for idx in con_sorted_idx]\n",
    "    # for idx in sorted_idx:\n",
    "    #     word = tfidf.get_feature_names()[idx]\n",
    "    #     control_keyword.append(word)\n",
    "\n",
    "     # ---------- Depression ----------\n",
    "    dep_sorted_idx = feat[1].argsort()[::-1]\n",
    "    depression_keyword = [tfidf.get_feature_names()[idx] for idx in dep_sorted_idx]\n",
    "    # for idx in sorted_idx:\n",
    "    #     word = tfidf.get_feature_names()[idx]\n",
    "    #     depression_keyword.append(word)\n",
    "\n",
    "    return depression_keyword, control_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742882\n",
      "4697533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "depression_keyword, control_keyword = get_tfidf_keyword(depression_text, control_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "['lsd', 'dose', 'leviticus', 'mdma', 'pregnancy', 'tripping', 'meds', 'substance', 'tolerance', 'shrooms']\n"
     ]
    }
   ],
   "source": [
    "indep_notincon = [word for word in depression_keyword[:5477] if word not in control_keyword[:5477]]\n",
    "\n",
    "print(len(indep_notincon))\n",
    "print(indep_notincon[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "['cmbeezy', 'el', 'en', 'es', 'shipped', 'supreme', 'turgle', 'mitch', 'kosovo', 'capitalism']\n"
     ]
    }
   ],
   "source": [
    "incon_notindep = [word for word in control_keyword[:5477] if word not in depression_keyword[:5477]]\n",
    "\n",
    "print(len(incon_notindep))\n",
    "print(incon_notindep[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./02-tfidf-depcon{save_top_words}-R{classi_ratio}-nostops.txt\", \"w\") as f:\n",
    "    for word in indep_notincon:\n",
    "        f.write(word + \"\\n\")\n",
    "    for word in incon_notindep:\n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54ebe46dc67c0b0016e368835037c988a8dce633f341e79a61a84613b212514"
  },
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
