{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd8238c-613e-40bb-8ec9-e27583fd106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "112\n",
      "3000\n",
      "26526\n",
      "22966\n"
     ]
    }
   ],
   "source": [
    "logodds_path = \"./01-logodds-topbot1500-R1-nostops.txt\"\n",
    "tfidf_path   = \"./02-tfidf-depcon3000-R1-nostops.txt\"\n",
    "lexicon_path = \"./03-depression-lexicon.txt\"\n",
    "sumatt_path  = \"./04-sum-attention-3000.txt\"\n",
    "topatt_path  = \"./topatt_masked_words_train.txt\"\n",
    "nn_path      = \"./nn_masked_words_train.txt\"\n",
    "\n",
    "\n",
    "with open(logodds_path) as f:\n",
    "    logodds = f.readlines()\n",
    "    logodds = [ word[:-1] for word in logodds]\n",
    "    print(len(logodds))\n",
    "    \n",
    "with open(tfidf_path) as f:\n",
    "    tfidf = f.readlines()\n",
    "    tfidf = [ word[:-1] for word in tfidf]\n",
    "    print(len(tfidf))\n",
    "    # print(tfidf)\n",
    "    \n",
    "with open(lexicon_path) as f:\n",
    "    lexicon = f.readlines()\n",
    "    lexicon = [ word[:-1] for word in lexicon]\n",
    "    print(len(lexicon))\n",
    "    # print(lexicon)\n",
    "    \n",
    "with open(sumatt_path) as f:\n",
    "    sumatt = f.readlines()\n",
    "    sumatt = [ word[:-1] for word in sumatt]\n",
    "    print(len(sumatt))\n",
    "    # print(sumatt)\n",
    "    \n",
    "with open(topatt_path) as f:\n",
    "    topatt = f.readlines()\n",
    "    topatt = [ word[:-1] for word in topatt]\n",
    "    print(len(topatt))\n",
    "    \n",
    "with open(nn_path) as f:\n",
    "    nn = f.readlines()\n",
    "    nn = [ word[:-1] for word in nn]\n",
    "    print(len(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38250d63-37f5-4552-9944-f113e6b76cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772\n",
      "21754\n"
     ]
    }
   ],
   "source": [
    "kw_tokens = [word for word in logodds if '#' in word]\n",
    "kw_words  = [word for word in logodds if '#' not in word]\n",
    "print(len(kw_tokens))\n",
    "print(len(kw_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed484c21-133b-4382-b495-c99e6a8f05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PUNCT': 11, 'CCONJ': 10, 'PRON': 43, 'DET': 22, 'ADV': 608, 'NOUN': 6334, 'AUX': 15, 'SCONJ': 19, 'PROPN': 8585, 'INTJ': 78, 'X': 206, 'VERB': 3181, 'ADP': 60, 'NUM': 794, 'ADJ': 1785, 'PART': 2, 'SYM': 1}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "count_pos_spacy = {}\n",
    "\n",
    "for word in kw_words:\n",
    "    sen = sp(word)\n",
    "    tag = sen[0].pos_\n",
    "    word = sen[0]\n",
    "    # print(sen[0], sen[0].pos_ ) #, sen[0].tag_)\n",
    "    \n",
    "    if tag not in count_pos_spacy.keys():\n",
    "        count_pos_spacy[tag] = 1\n",
    "    else : \n",
    "        count_pos_spacy[tag] += 1\n",
    "    \n",
    "print(count_pos_spacy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
