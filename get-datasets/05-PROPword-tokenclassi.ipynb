{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461e6306-bb1f-4373-b75d-c3a01036fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache/'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.dataset import *\n",
    "from src.utils   import *\n",
    "from src.traineval  import *\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586628e5-3d2d-472e-997d-de6985615b71",
   "metadata": {},
   "source": [
    "## 01/06 Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c3e4e-b795-450a-b1d6-f05453f11bf5",
   "metadata": {},
   "source": [
    "- Get correctly classified samples from depression and control class\n",
    "- Get the top 76 words by word's average attention score in each sample\n",
    "- Masked those words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c760eb-c530-41d2-b602-54d8ab452441",
   "metadata": {},
   "source": [
    "# Create Token classi dataset from classi train R1 dataset\n",
    "\n",
    "### Mask top 76 tokens with highest attention score in each correctly classified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddce7b20-78e7-4e00-aebb-c91b39d3bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from  ../save/BASE-classiCEr1/best-model-1500.tar\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "load_model_from = '../save/BASE-classiCEr1/best-model-1500.tar'\n",
    "\n",
    "checkpoint           = 'bert-base-uncased'\n",
    "\n",
    "training_obj       = 'classiCE'\n",
    "\n",
    "classifier_p_dropout = 0.1\n",
    "\n",
    "num_labels = 2 #CE\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels).to(device)\n",
    "model.classifier.dropout = nn.Dropout(p = classifier_p_dropout, inplace = False)\n",
    "\n",
    "print(\"Load from \", load_model_from)\n",
    "checkpoint = torch.load(load_model_from)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(model.load_state_dict(checkpoint['model_state_dict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda554d4-443b-4e6a-a302-e10b0c5a56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedTokenClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, model, device):\n",
    "        \n",
    "        self.make_label_tokenclassification(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'input_ids'    : self.all_input_ids[idx],\n",
    "                'attention_mask' : torch.ones_like(self.all_input_ids[idx]),\n",
    "                'labels'         : self.all_labels[idx],\n",
    "                'text'           : self.all_text[idx]\n",
    "                 }\n",
    "        return sample\n",
    "    \n",
    "    # Use important_words_info to make the label for training TokenClassification on this D(task) dataset\n",
    "    def make_label_tokenclassification(self, dataset):\n",
    "        all_input_ids = []\n",
    "        all_labels    = []\n",
    "        all_text      = []\n",
    "        \n",
    "        \n",
    "        for idx , data in enumerate(dataset):\n",
    "            sys.stdout.write(str(idx))\n",
    "            \n",
    "            orig_input_ids  = data['input_ids'].to(device).reshape(1,-1)\n",
    "            orig_labels     = data['labels']\n",
    "            \n",
    "            label          = torch.zeros_like(orig_input_ids).cpu()\n",
    "            \n",
    "            # ---------------- MKR -----------------\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs   = model(orig_input_ids, output_attentions=True)        \n",
    "                attention = outputs.attentions[-1].sum(dim = 1).detach().cpu() # get attention of last layer (1, 12, 512, 512) >> (1, 512, 512)\n",
    "            # print(attention.shape)\n",
    "\n",
    "            # get attention from only CORRECTLY classified samples\n",
    "            pred = int(torch.argmax(torch.softmax(outputs.logits.detach().cpu(), dim = 1), dim = 1))\n",
    "            correct = pred == orig_labels\n",
    "            # print(correct)\n",
    "\n",
    "            if correct :   \n",
    "                \n",
    "                word_ids = data['word_ids']\n",
    "                word_score_dict = {}\n",
    "                \n",
    "                # print(word_ids)\n",
    "                # print(orig_input_ids)\n",
    "                # print(attention[0,0,:])\n",
    "                \n",
    "                words_score = {} # k = index of word : v = avg score of word\n",
    "                for word_id in word_ids[1:-1]:\n",
    "                    idx_this_word = torch.tensor([input_idx for input_idx in range(1,orig_input_ids.shape[1]-1) if word_id == word_ids[input_idx]])\n",
    "                    \n",
    "                    if idx_this_word.size()[0] > 1:\n",
    "                        # print(int(idx_this_word[0]), int(idx_this_word[-1]))\n",
    "                        avg_word_score = attention[0, 0, int(idx_this_word[0]) : int(idx_this_word[-1])+1]\n",
    "                        # print(avg_word_score)\n",
    "                    else:\n",
    "                        avg_word_score = attention[0, 0, int(idx_this_word)]\n",
    "                        # print(avg_word_score)\n",
    "                    \n",
    "                    # print(avg_word_score)\n",
    "                    avg_word_score = torch.mean(avg_word_score)\n",
    "                    # print(avg_word_score.shape)\n",
    "                    # print(avg_word_score)\n",
    "                    \n",
    "                    words_score[idx_this_word] = avg_word_score\n",
    "                    # print(words_score)\n",
    "                \n",
    "                sorted_words_score = sorted(words_score.items(), key=lambda x: x[1], reverse=True)   \n",
    "                # print(sorted_words_score)\n",
    "                \n",
    "                top_indices = []\n",
    "                for index, score in sorted_words_score:\n",
    "                    top_indices.extend(index.tolist())\n",
    "                    if len(top_indices) >= 76:\n",
    "                        break\n",
    "                # print(len(top_indices))\n",
    "            \n",
    "                label.index_fill_(dim=1, index = torch.tensor(top_indices, dtype=torch.int64), value = 1)\n",
    "            \n",
    "            # first and last position of label should be 0 (CLS and SEP)\n",
    "            label[0,0]    = 0\n",
    "            label[0,-1]   = 0\n",
    "\n",
    "            all_input_ids.append(data['input_ids'].squeeze(0))\n",
    "            all_labels.append(label.squeeze(0))\n",
    "            all_text.append(data['orig_text'])       \n",
    "            \n",
    "        self.all_input_ids = all_input_ids\n",
    "        self.all_labels    = all_labels\n",
    "        self.all_text      = all_text\n",
    "        \n",
    "        del all_input_ids, all_labels, all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51a3d96-5473-4ee1-b503-5cb7fdabe0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234567891011121314151617181920212223242526272829303132333435363738394041424344454647"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a3283b15097b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenclassi_all_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProposedTokenClassificationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenclassi_all_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../keyword/PROP-tokenclassi-R1dataset.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-54c4d4b37d10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, model, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_tokenclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-54c4d4b37d10>\u001b[0m in \u001b[0;36mmake_label_tokenclassification\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mwords_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# k = index of word : v = avg score of word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0midx_this_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morig_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0midx_this_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-54c4d4b37d10>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mwords_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# k = index of word : v = avg score of word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0midx_this_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morig_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0midx_this_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open(f'../data/classi/classichunk-R1-train-ds.pkl', \"rb\"))\n",
    "\n",
    "tokenclassi_all_ds = ProposedTokenClassificationDataset(train_dataset, model, device)\n",
    "\n",
    "print(len(tokenclassi_all_ds))\n",
    "\n",
    "with open('../keyword/PROP-tokenclassi-R1dataset.pkl', 'wb') as outp:\n",
    "    pickle.dump(tokenclassi_all_ds, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c4684a-83d8-4d63-89f2-0d18cada3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenclassi_all_ds = pickle.load(open('./PROP-tokenclassi-R1dataset.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5a47e8-b5e5-4005-be20-cb4fe24a42c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2009, 28043,  2026,  2969, 19593,  2004,  2092,  2000,  2025,\n",
      "         2022,  2197,  1012,  1045,  2001,  7087,  6340,  1037,  2843,  1010,\n",
      "         2130,  2011,  2070,  1997,  1996,  3057,  2012,  2335,  2021,  2008,\n",
      "         2028,  2404,  1037,  1059, 24475,  1010,  1045,  1005,  1049,  2917,\n",
      "        18804,  5302, 14536, 24158,  1029,  2000,  1037,  2261,  1997,  1996,\n",
      "         4364,  1012,  1045,  2001,  2404,  4873,  1999,  1996,  2690,  2021,\n",
      "         2145,  1012,  1045,  2823,  2018, 11519, 12845,  2015,  1012,  2009,\n",
      "         2001,  2130,  6057,  2012,  2335,  2007,  2122, 12064,  2067,  1005,\n",
      "         1050,  1005, 15421,  2021,  2009,  2001,  2467,  1037, 18503,  1012,\n",
      "         2065,  2027,  2134,  1005,  1056,  2031,  1037, 12845,  2027,  2823,\n",
      "         2357,  3558,  2612,  1012,  1045,  2444,  1999,  5120,  1012,  1045,\n",
      "         2228,  2111, 28616, 20824, 21515,  2054,  1045,  2812,  2007,  3147,\n",
      "         1010,  1045,  2812,  2111,  2040, 19233,  4555,  8292,  4877,  4173,\n",
      "         2030,  2917,  2503,  1998,  8509,  1996,  1996, 10867, 28696,  2102,\n",
      "         9951,  2135,  2659,  1010,  2025,  2074,  3621,  2917,  3671,  1012,\n",
      "         1045,  2453,  2036,  2031, 28947,  1037,  2261, 21392, 27469,  2015,\n",
      "        12157,  2030,  2054,  2027,  2812,  2004,  2092,  2349,  2000,  2023,\n",
      "         1012,  1045,  2175,  4586, 21172,  1999,  1996,  3467,  2823,  2061,\n",
      "         2009,  1005,  1055,  2025,  2066,  1045,  2031,  2784,  7385, 12707,\n",
      "         5596,  2005,  1037,  2210,  2978,  1997, 23362,  2250,  2021,  2788,\n",
      "         2994,  2503,  2004,  2172,  2004,  1045,  2064,  2076,  1996,  3467,\n",
      "         2004,  2043,  1045,  2034,  2707, 12809,  2009,  3138,  2070,  2051,\n",
      "         2077,  1045,  2131,  4010,  2153,  1012,  1045,  2079,  2224,  2062,\n",
      "         4253,  2084,  2500,  2349,  2000,  2009,  2021,  2009,  1005,  1055,\n",
      "         2025,  2066,  1045, 17612,  2065,  7955,  2842,  2003,  2986,  2007,\n",
      "         1056,  3351,  4860,  1012,  2021,  2008,  2711,  1999,  1996,  2436,\n",
      "         2467,  5278,  1996,  1996, 10867, 28696,  2102,  2096,  3564,  1999,\n",
      "         1037, 14329,  5754, 27153,  2033,  1010,  2026,  2767, 21393,  8292,\n",
      "         4877,  4173,  2012,  2188,  2788,  3084,  2033,  5665,  2044,  1037,\n",
      "         2261,  2420,  2004,  1045,  6719,  2203,  2039, 12809,  2005,  2146,\n",
      "         6993,  1997,  2051,  2130,  2007, 14329, 19022,  5802,  3489,  3215,\n",
      "         1998,  4933,  1012,  2009,  1005,  1040,  2022,  3835,  2000,  2424,\n",
      "         1037,  2690,  2598,  1998,  6293,  2007,  2009,  2061,  1045,  2064,\n",
      "         2031,  2026, 12121, 14329,  2006,  1998,  2022,  2986,  2096,  2002,\n",
      "         7719,  1999,  1037, 24529, 11961,  2102,  1012,  1045,  3984,  2017,\n",
      "         2123,  1005,  1056,  2903,  2033,  1010,  1998,  2008,  1005,  1055,\n",
      "         2986,  1012,  1045,  2113,  1010,  1045,  9544,  2009,  1037,  3232,\n",
      "         5445,  4921,  2121,  3671,  2282,  4860,  1012,  2023, 12043,  1998,\n",
      "         2010,  2567,  2027,  9278,  2362,  2562,  1996,  3645,  2330,  2012,\n",
      "         9428,  2035,  2335,  1010,  2130,  2076,  1996,  3467,  1012,  2009,\n",
      "         1005,  1055,  2196,  2008,  3147,  1999,  1996,  2436,  2021,  2028,\n",
      "         1997,  1996,  2111,  2045,  7777,  2009,  3147,  1010,  5525,  2126,\n",
      "         2205,  3147,  2005,  2033,  2040,  7906,  1037, 14329,  2006,  2505,\n",
      "         2012,  2030,  3621,  2917,  3671,  8915,  8737,  1012,  1045,  1005,\n",
      "         1049, 25930,  2102,  1996,  2069,  2028,  2040,  1005,  1055, 10865,\n",
      "         2055,  2009,  2045,  2021,  2012,  2560,  2009,  1005,  1055,  2444,\n",
      "         3085,  1999,  7831,  1012,  1045,  2123,  1005,  1056,  2113,  2054,\n",
      "         4860,  1061,  1005,  2035,  2031,  1999,  2568,  2005,  2108,  3147,\n",
      "         2030, 23362,  2030,  9608,  2000,  6752,  2007,  1996,  1996, 10867,\n",
      "        28696,  2102,  2005,  2295,  1012,  2672,  1045,  1005,  1049,  2074,\n",
      "         4060,  2100,  2021,  2009,  2469,  2004,  3109, 19237,  2000, 13184,\n",
      "         2035,   102])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(tokenclassi_all_ds):\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['labels'])\n",
    "    print(sample['attention_mask'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0c9d1-86a5-47b4-bd60-8288abbb6b74",
   "metadata": {},
   "source": [
    "# Train Token Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c327be-4d01-4057-901a-6d676d4ef057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3704\n",
      "463\n",
      "464\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE  = 0.1\n",
    "\n",
    "len_ds = len(tokenclassi_all_ds)\n",
    "\n",
    "all_idx = list(range(len_ds))\n",
    "random.seed(42)\n",
    "random.shuffle(all_idx)\n",
    "\n",
    "train_indices = all_idx [ : int(len_ds*(1-TEST_SIZE-VAL_SIZE)) ]\n",
    "val_indices   = all_idx [ int(len_ds*(1-TEST_SIZE-VAL_SIZE)) : int(len_ds*(1-TEST_SIZE))]\n",
    "test_indices  = all_idx [ int(len_ds*(1-TEST_SIZE)) : ]\n",
    "\n",
    "# generate subset based on indices\n",
    "train_dataset = Subset(tokenclassi_all_ds, train_indices)\n",
    "val_dataset   = Subset(tokenclassi_all_ds, val_indices)\n",
    "test_dataset  = Subset(tokenclassi_all_ds, test_indices)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1eb14e-955b-4446-99d2-336f8b6a84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "tokenizer_checkpoint = 'bert-base-uncased'\n",
    "\n",
    "training_obj       = 'classitoken'\n",
    "masking_method     = None\n",
    "keyword_path       = None \n",
    "\n",
    "classifier_p_dropout = 0.1\n",
    "\n",
    "learning_rate = 5e-6\n",
    "batch_size    = 32\n",
    "num_epoch     = 100\n",
    " \n",
    "val_steps     = 200\n",
    "logging_steps = 50\n",
    "max_file_save = 5\n",
    "\n",
    "\n",
    "if debug == True:\n",
    "    save_dir      = f'../save/debug-PROP-{training_obj}'\n",
    "    writer        = SummaryWriter(log_dir = f'../tensorboard_runs/FINALv1_runs/debug-PROP-{training_obj}')\n",
    "    num_epoch     = 2\n",
    "    val_steps     = 10\n",
    "    logging_steps = 10\n",
    "else:\n",
    "    save_dir      = f'../save/PROP-{training_obj}'\n",
    "    writer        = SummaryWriter(log_dir = f'../tensorboard_runs/FINALv1_runs/PROP-{training_obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a49a61-65e5-44d8-ad07-eb6e4de3afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if training_obj == 'classitoken' :\n",
    "    num_labels  = 2 # CE\n",
    "    model       = AutoModelForTokenClassification.from_pretrained(tokenizer_checkpoint, num_labels = num_labels).to(device)\n",
    "    model.classifier.dropout = nn.Dropout(p = classifier_p_dropout, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0fd3d7e-1089-4b93-91a9-9eb50d52e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalLoop():\n",
    "    \n",
    "   # Token classification : use outputs.loss    \n",
    "    @classmethod      \n",
    "    def train_one_step_tokenclassi(cls, data):\n",
    "        \n",
    "            cls.current_step += 1\n",
    "            \n",
    "            input_ids = data['input_ids'].to(cls.device)\n",
    "            att_mask  = data['attention_mask'].to(cls.device)\n",
    "            labels    = data['labels'].to(cls.device).long()\n",
    "\n",
    "            cls.optimizer.zero_grad()\n",
    "\n",
    "            outputs = cls.model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "            # classification\n",
    "            logits = outputs.logits # (before SoftMax)\n",
    "            loss   = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            cls.optimizer.step()\n",
    "            \n",
    "            cls.all_losses['train_loss'].update(loss.item(), input_ids.shape[0])\n",
    "                          \n",
    "            # if it is the eval step then cal Loss and Metrics of THIS STEP\n",
    "            if (cls.current_step % cls.val_steps == 0 or cls.current_step % cls.logging_steps == 0) and cls.current_step != 0:\n",
    "                # record TRAIN loss\n",
    "                cls.writer.add_scalar(tag = \"train/loss\", scalar_value = loss.item(), global_step = cls.current_step)\n",
    "                print(f\"Epoch : {cls.current_epoch} | Step : {cls.current_step}\")\n",
    "                print(\"Train Loss    : \" , loss.item())\n",
    "                # TRAIN metrics\n",
    "                cls.all_metrics['train_metrics'].keep(torch.softmax(logits, dim = 2), labels) # bc our logits did not pass sigmoid or softmax yet\n",
    "                cls.all_metrics['train_metrics'].calculate(cls.current_step, cls.writer, 'train', cls.training_obj)\n",
    "        \n",
    "    # Token classification : use outputs.loss       \n",
    "    @classmethod\n",
    "    def evaluate_tokenclassi(cls, mode):\n",
    "        cls.all_losses[f'{mode}_loss'].reset()\n",
    "        \n",
    "        if mode == 'test':\n",
    "            data_loader = cls.test_loader\n",
    "        if mode == 'val':\n",
    "            data_loader = cls.val_loader\n",
    "            \n",
    "        for data in data_loader:    \n",
    "                \n",
    "            # Every data instance is an input + label pair\n",
    "            input_ids = data['input_ids'].to(cls.device)\n",
    "            att_mask  = data['attention_mask'].to(cls.device)\n",
    "            labels    = data['labels'].to(cls.device).long()\n",
    "\n",
    "            if mode == 'val':\n",
    "                outputs = cls.model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "            if mode == 'test':\n",
    "                outputs = cls.best_model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "            # classification\n",
    "            logits = outputs.logits # (before SoftMax)\n",
    "            loss   = outputs.loss\n",
    "            \n",
    "            cls.all_losses[f'{mode}_loss'].update(loss.item(), input_ids.shape[0])\n",
    "            cls.all_metrics[f'{mode}_metrics'].keep(torch.softmax(logits, dim = 2), labels) # bc our logits did not pass sigmoid or softmax yet    \n",
    "        \n",
    "        # report Loss and Metrics of the Whole VAL SET\n",
    "        print(f\"{mode} Loss    : \" , cls.all_losses[f'{mode}_loss'].average)\n",
    "        cls.writer.add_scalar(tag = f\"{mode}/loss\", scalar_value = cls.all_losses[f'{mode}_loss'].average, global_step = cls.current_step)\n",
    "        cls.all_metrics[f'{mode}_metrics'].calculate(cls.current_step, cls.writer, mode, cls.training_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0adca74e-28f4-44b9-a084-053e884641ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "training_arg = {\n",
    "                'device'       : device, \n",
    "                'model'        : model,\n",
    "                'training_obj' : training_obj, \n",
    "                'criterion'    : None,\n",
    "                'optimizer'    : optimizer,\n",
    "                'batch_size'   : batch_size,\n",
    "                'num_epoch'    : num_epoch,\n",
    "                \n",
    "                'train_dataset' : train_dataset,\n",
    "                'val_dataset'   : val_dataset, \n",
    "                'test_dataset'  : test_dataset,\n",
    "                \n",
    "                'val_steps'     : val_steps,\n",
    "                'logging_steps' : logging_steps, \n",
    "                'save_dir'      : save_dir,\n",
    "    \n",
    "                'max_file_save' : max_file_save,\n",
    "                'writer'        : writer,\n",
    "                \n",
    "                'load_model_from' : None, # saved model path\n",
    "}\n",
    "\n",
    "TrainEval = TrainEvalLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aadb8eaf-7427-4473-bbee-5ddf4b02118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer():\n",
    "    def __init__(self, training_arg, TrainEval):\n",
    "        \n",
    "        self.load_model_from = training_arg['load_model_from']\n",
    "        \n",
    "        self.train_dataset = training_arg['train_dataset']\n",
    "        self.val_dataset   = training_arg['val_dataset']\n",
    "        self.test_dataset  = training_arg['test_dataset']\n",
    "        \n",
    "        self.device        = training_arg['device']\n",
    "        self.model         = training_arg['model']\n",
    "        self.training_obj  = training_arg['training_obj']\n",
    "        self.criterion     = training_arg['criterion']\n",
    "        self.optimizer     = training_arg['optimizer']\n",
    "        self.batch_size    = training_arg['batch_size']\n",
    "        self.num_epoch     = training_arg['num_epoch']\n",
    "        \n",
    "        self.val_steps     = training_arg['val_steps']\n",
    "        self.logging_steps = training_arg['logging_steps']\n",
    "        self.save_dir      = training_arg['save_dir']\n",
    "        self.max_file_save = training_arg['max_file_save']\n",
    "        self.writer        = training_arg['writer']\n",
    "        self.best_model    = None\n",
    "        \n",
    "        CHECK_FOLDER = os.path.isdir(self.save_dir)\n",
    "        # If folder doesn't exist, then create it.\n",
    "        if not CHECK_FOLDER:\n",
    "            os.makedirs(self.save_dir)\n",
    "            print(\"created folder : \", self.save_dir)\n",
    "        \n",
    "        self.create_dataloaders()\n",
    "        \n",
    "        self.all_losses = dict()\n",
    "        self.all_losses['train_loss'] = AverageMeter()\n",
    "        self.all_losses['val_loss']   = AverageMeter()\n",
    "        self.all_losses['test_loss']  = AverageMeter()\n",
    "        \n",
    "        if training_obj == 'classitoken':\n",
    "            self.all_metrics = dict()\n",
    "            self.all_metrics['train_metrics'] = ClassiMetricMeter()\n",
    "            self.all_metrics['val_metrics']   = ClassiMetricMeter()\n",
    "            self.all_metrics['test_metrics']  = ClassiMetricMeter()\n",
    "            self.train_one_step = TrainEval.train_one_step_tokenclassi\n",
    "            self.evaluate       = TrainEval.evaluate_tokenclassi\n",
    "            print(\"Training Token Classification with Huggingface's Loss...\")\n",
    "            \n",
    "        if self.load_model_from is not None:\n",
    "            self.load_checkpoint()\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def create_dataloaders(self, ):\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True, pin_memory=True)\n",
    "        del self.train_dataset\n",
    "        self.val_loader   = DataLoader(self.val_dataset,   batch_size = self.batch_size, shuffle = True, pin_memory=True)\n",
    "        del self.val_dataset\n",
    "        self.test_loader  = DataLoader(self.test_dataset,  batch_size = self.batch_size, shuffle = True, pin_memory=True)\n",
    "        del self.test_dataset\n",
    "        \n",
    "        self.total_steps  = int(self.num_epoch * len(self.train_loader))\n",
    "        print(f\"{self.num_epoch} epochs = {self.total_steps} steps\")\n",
    "    \n",
    "    def train(self,):\n",
    "        self.best_step  = 0\n",
    "        self.best_model = copy.deepcopy(self.model)\n",
    "        tmp_val_loss = 10e+9\n",
    "        \n",
    "        for _ in range(self.num_epoch): # for epoch\n",
    "            \n",
    "            for i, data in enumerate(self.train_loader): # for data in data_loader\n",
    "                sys.stdout.write(str(i))\n",
    "                \n",
    "                self.model.train(True)\n",
    "                self.train_one_step.__func__(self, data)\n",
    "                \n",
    "                # do evaluation every val_steps\n",
    "                if self.current_step % self.val_steps == 0 and self.current_step != 0:\n",
    "                    \n",
    "                    self.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        self.evaluate.__func__(self, 'val')\n",
    "                    \n",
    "                    self.writer.flush()\n",
    "                    self.save_checkpoint()\n",
    "                    \n",
    "                    curr_val_loss = self.all_losses['val_loss'].average\n",
    "                    \n",
    "                    # keep a copy of the current best model\n",
    "                    if curr_val_loss < tmp_val_loss:\n",
    "                        clear_output(wait=True)\n",
    "                        tmp_val_loss    = curr_val_loss\n",
    "                        self.best_step  = self.current_step\n",
    "                        self.best_model = copy.deepcopy(self.model)\n",
    "                        print(\"Best step : \", self.best_step, \"Val loss at Best Step : \",  curr_val_loss)\n",
    "                        \n",
    "                \n",
    "            self.all_losses['train_loss'].reset()  # for train loss we reset every epoch \n",
    "            self.current_epoch += 1\n",
    "        \n",
    "        self.save_best_model()\n",
    "        self.writer.close()\n",
    "        print(\"==================== Finished Training !!! =======================\")\n",
    "        \n",
    "        self.inference()\n",
    "        \n",
    "        \n",
    "    def inference(self,): # test with best model\n",
    "        self.best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.evaluate.__func__(self, 'test')\n",
    "            \n",
    "    def save_best_model(self, ):\n",
    "        torch.save({\n",
    "            'step'                : self.best_step,\n",
    "            'model_state_dict'    : self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss'          : self.all_losses['train_loss'].average,\n",
    "            'val_loss'           : self.all_losses['val_loss'].average,\n",
    "            \n",
    "            }, f'{self.save_dir}/best-model-{self.best_step}.tar')\n",
    "    \n",
    "          \n",
    "    def save_checkpoint(self,):\n",
    "        \n",
    "        if int(self.current_step - (self.val_steps*self.max_file_save)) > 0:\n",
    "            os.remove(f'{self.save_dir}/checkpoint-{ int(self.current_step - (self.val_steps*self.max_file_save))}.tar')\n",
    "        \n",
    "        torch.save({\n",
    "            'step'                : self.current_step,\n",
    "            'model_state_dict'    : self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss'          : self.all_losses['train_loss'].average,\n",
    "            'val_loss'            : self.all_losses['val_loss'].average,\n",
    "            \n",
    "            }, f'{self.save_dir}/checkpoint-{self.current_step}.tar')\n",
    "        \n",
    "    def load_checkpoint(self,): # only for continuing the next task (NOT for continue training)\n",
    "        print(\"Load from \", self.load_model_from)\n",
    "        checkpoint = torch.load(self.load_model_from)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(self.model.load_state_dict(checkpoint['model_state_dict'])) # <All keys matched successfully>\n",
    "        \n",
    "        # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        # self.current_step = checkpoint['step']\n",
    "        # loss = checkpoint['train_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2036bcd-2885-43c0-90e0-a9bd334ea3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder :  ../save/PROP-classitoken\n",
      "50 epochs = 5800 steps\n",
      "Training Token Classification with Huggingface's Loss...\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(training_arg, TrainEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f805e-cd0b-4b14-8d1f-c460a313f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
